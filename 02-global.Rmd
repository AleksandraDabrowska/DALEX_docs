# Model understanding  {#modelUnderstanding}

In this chapter we will introduce three groups of explainers that can be used to boost our understanding of black-box models.

* Section \@ref(modelPerformance) presents explainers for model performance. Single number may be misleading when we need to compare complex models. Here you will find plots that give more information about model performance in a consistent form.
* Section \@ref(featureImportance) presents explainers for variable importance. Knowing which variables are important allow to validate the model and increase our understanding of the domain.
* Section \@ref(variableResponse) presents explainers for variable effect. Here you will find plots that summaries the relation between model response and particular variable.

All explainers are illustrated based on two models fitted to the `apartments` data.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
apartments_lm_model <- lm(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)
library("randomForest")
set.seed(59)
apartments_rf_model <- randomForest(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)
```

First we need to prepare wrappers for these models. They will be available in `explainer_lm` and `explainer_rf` objects.

```{r, warning=FALSE, message=FALSE}
explainer_lm <- explain(apartments_lm_model, 
                       data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)
explainer_rf <- explain(apartments_rf_model, 
                       data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)
```

## Model performance {#modelPerformance}

As you may remember from previous chapter [root mean square](https://en.wikipedia.org/wiki/Root_mean_square) of residuals is similar for both considered models. Does it mean that these models are equally good?

```{r, warning=FALSE, message=FALSE}
predicted_mi2_lm <- predict(apartments_lm_model, apartmentsTest)
sqrt(mean((predicted_mi2_lm - apartmentsTest$m2.price)^2))

predicted_mi2_rf <- predict(apartments_rf_model, apartmentsTest)
sqrt(mean((predicted_mi2_rf - apartmentsTest$m2.price)^2))
```

Function `model_performance()` calculates predictions for validation `data` and differences between model predictions and supplied labels `y`.

Generic function `print()` returns quantiles for these differences.

```{r}
mp_lm <- model_performance(explainer_lm)
mp_rf <- model_performance(explainer_rf)
mp_lm
mp_rf
```

The generic `plot()` function shows reversed empirical cumulative distribution function for absolute values from quintiles. This function presents fraction of residuals larger than `x`. The figure below shows that majority of residuals for random forest is smaller than residuals in linear model. But the small fraction of very large residuals affects the root mean square.

```{r global_explain_ecdf, fig.cap="Comparison of residuals for linear model and random forest"}
plot(mp_lm, mp_rf)
```

Use the `geom = "boxplot"` parameter for the generic `plot()` function to get alternative comparison of residuals. The red dot here stands for the root mean square.

```{r global_explain_boxplot, fig.height=2.5, fig.cap="Comparison of residuals for linear model and random forest"}
plot(mp_lm, mp_rf, geom = "boxplot")
```


## Feature importance {#featureImportance}

Explainers presented in this section are designed to better understand which variables are the most important? 

Some models, like linear regression or random forest have build-in *model specific* methods to calculate are visualize variable importance. They will be presented in Section \@ref(modelSpecific).

Section \@ref(modelAgnostic) presents a model agnostic approach based on permutations. The advantage of this approach is that different models can be compare within a single setup.


### Model agnostic {#modelAgnostic}

Model agnostic variable importance is calculated via permutations. Simply the loss function is compared between the full model and the model with single variable being permuted. The concept along with some extensions is described in [@variableImportancePermutations].

In DALEX this method is implemented in the `variable_importance()` function. Loss function is calculated for a model

* based on validation `data`, this is an estimate of model performance (will be denoted as `_full_model_`),
* based on a data with resampled labels `y`, this is kind of *worst case* loss when model are compares against random labels (will be denoted as `_baseline_`)
* based on a data with single variable being resampled, this will tell us how much model looses when selected variable is blinded.


Let's see how this function is working for a random forest model. 

```{r}
vi_rf <- variable_importance(explainer_rf, loss_function = loss_root_mean_square)
vi_rf
```

<p><span class="marginnote">Here the `loss_root_mean_square()` function is defined as square root from averaged squared differences between labels and model predictions.
</span>
Same method may be applied to linear model. Since we are using same loss function and same method for variable permutations then losses calculated with both methods can be directly compared.</p>


```{r}
vi_lm <- variable_importance(explainer_lm, loss_function = loss_root_mean_square)
vi_lm
```

It is much easier to compare both models when these values are plotted.
The generic `plot()` function may handle both models. 

```{r modelImportanceRaw, message=FALSE, warning=FALSE, fig.height=3.5, fig.cap="Model agnostic variable importance plot. Right edges correspond to loss function after permutation of a single variable. Left edges correspond to loss of a full model"}
plot(vi_lm, vi_rf)
```

What we can read out of this plot?

* left edges of intervals start in `_full_model_` losses, and as we see the performances are similar for both models
* length of the interval correspond to variable importance. In both models the most important variables are `district` and `surface`
* in the random forest model the `construction_year` variable has some importance while for linear model its importance is almost zero
* the variable `no.rooms` (which is correlated with `surface`) has some importance in the random forest model but not in the linear model.

We may be interested in variables that behave differently between models (like `construction_year`) or variables that are important (like `district` or `surface`). In the next section we will introduce explainers for further investigations.


*NOTE:* If you like to have all intervals hooked to 0, you can do this as well. Just add `type = "difference"` parameter to `variable_importance()`.

```{r modelImportanceDifference, message=FALSE, warning=FALSE, fig.height=3.5, fig.cap="Model agnostic variable importance plot. Right edges correspond to difference between loss after permutation of a single variable and loss of a full model"}
vi_lm <- variable_importance(explainer_lm, loss_function = loss_root_mean_square, type = "difference")
vi_rf <- variable_importance(explainer_rf, loss_function = loss_root_mean_square, type = "difference")
plot(vi_lm, vi_rf)
```


### Model specific {#modelSpecific}

Some models have build-in tools for calculation of variable importance.
Random forest is using two different measures, one based on out-of-bag data and second based on gains in nodes. Read more about this approach in [@randomForest]. 

Below we show an example of dot plot that summarizes default importance measure for random forest. The `varImpPlot()` function is available in the `randomForest` package.


```{r modelImportanceRF, message=FALSE, warning=FALSE, fig.height=3.5, fig.cap="Built-in variable importance plot for random forest"}
varImpPlot(apartments_rf_model)
```

It is also straightforward to assess variable importance for linear models and generalized models. It is simple since model coefficients have direct interpretation.

[Forest plots](https://en.wikipedia.org/wiki/Forest_plot) were initially used in the meta analysis to visualise effects in different studies. But now they are frequently used to present summary characteristics for models with linear structure like these created with `lm` or `glm` functions.

There are various implementations of forest plots in R. In the package **forestmodel** (see [@forestmodel]) one can use `forest_model()` function to draw a forest plot. This package is based on the **broom** package (see [@broom]) and this is why it handles a large variety of different regression models. 

```{r forestmodel, warning=FALSE, message=FALSE, fig.width=10, fig.height=5, fig.cap='Forest plot created with forestmodel package'}
library("forestmodel")
forest_model(apartments_lm_model)
```

In the package **sjPlot** (see [@sjPlot]) one can find `sjp.*()` function to visualise coefficients of a `*` model (like `sjp.glm()` for `glm` models) or a wrapper `plot_model()`. 


```{r sjpglm, message=FALSE, warning=FALSE, fig.width=10, fig.width=8, fig.cap='Model coefficients plotted  with sjPlot package'}
library("sjPlot")
plot_model(apartments_lm_model, type = "est", sort.est = TRUE)
```

**Note!** 

The **forestmodel** package handles factor variables in a better way while the plots from **sjPlot** are easier to read.


## Variable response {#variableResponse}

Explainers presented in this section are designed to better understand the relation between a variable and model output.

Subsection \@ref(pdpchapter) presents Partial Dependence Plots (PDP), one of the most popular methods for exploration of a relation between a continuous variable and model outcome. 
Subsection \@ref(accumulatedLocalEffects) presents Accumulated Local Effects Plots (ALEP), an extension of PDP, more suited for highly correlated variables.

Subsection \@ref(mergingPathPlot) presents Merging Path Plots, a method for exploration of a relation between a categorical variable and model outcome.

### Partial Dependence Plot {#pdpchapter}

Partial Dependence Plots (see **pdp** package [@pdp]) for a black box $f(x; \theta)$ calculates the expected output given a selected variable.

$$
p_i(x_i) = E_{x_{-i}}[ f(x^i, x^{-i}; \theta) ]
$$

Of course this expectation cannot be calculated directly as we do not know fully the $f()$ neither the distribution of $x_{-i}$. This value is estimated by 

$$
\hat p_i(x_i) = \frac{1}{n} \sum_{j=1}^{n} f(x^i_j, x_j^{-i}, \hat \theta) 
$$


Let's see an example for the model `apartments_rf_model`. Below we are using `variable_response()` from `DALEX`, which is calling   `pdp::partial` function to calculate PDP response.

Section \@ref(featureImportance) shows variable importance plots for different models. The variable `construction.year` is interesting as it is important for the random forest model `apartments_rf_model` but not for the linear model `apartments_lm_model`. Let's see the relation between the variable and model output. 

```{r pdpRandomForest, message=FALSE, warning=FALSE, fig.cap="Relation between output from `apartments_rf_model` and variable `construction.year`"}
sv_rf  <- single_variable(explainer_rf, variable =  "construction.year", type = "pdp")
plot(sv_rf)
```

We can use PDP plots to compare two or more models. Here we are plotting PDP for the linear model against the random forest model.

```{r pdpRandomForestLM, message=FALSE, warning=FALSE, fig.cap="Relation between output from models `apartments_rf_model` and `apartments_lm_model` against the variable `construction.year`"}
sv_lm  <- single_variable(explainer_lm, variable =  "construction.year", type = "pdp")

plot(sv_rf, sv_lm)
```

It looks like the random forest captures non-linear relation that cannot be captured by linear models.


### Accumulated Local Effects Plot {#accumulatedLocalEffects}

As it is presented in section \@ref(pdpchapter), the Partial Dependence Plot presents the expected model response with respect to marginal distribution of $x_{-i}$. 
In some cases, e.g. when repressors are highly correlated, expectation over the marginal distribution may lead to biases/poorly extrapolated model responses. 

Accumulated local effects (ALE) plots (see **ALEPlot** package [@ALEPlot]) solves this problem by using conditional distribution $x_{-i}|x_i = x_i^*$. This leads to more stable and reliable estimates (at least when the predictors are highly correlated).

Estimation of main effects for `construction.year` is similar to the PDP curves. Here we are using `DALEX::single_variable` function that is calling   `ALEPlot::ALEPlot` function to calculate ALE curve for variable `construction.year`. 

```{r alePlotsRF, message=FALSE, warning=FALSE, fig.cap="Relation between output from models `apartments_rf_model` and `apartments_lm_model` against the variable `construction.year` calculated with Accumulated local effects."}
sva_rf  <- single_variable(explainer_rf, variable = "construction.year", type = "ale")
sva_lm  <- single_variable(explainer_lm, variable = "construction.year", type = "ale")

plot(sva_rf, sva_lm)
```

Results for PDP and ALEP are very similar except that effects for ALEP are centered around 0.


### Mering Path Plot {#mergingPathPlot}

The package `ICEbox` is not working for factor variables while the `pdp` package returns plots that are hard to interpret.

An interesting tool that helps to understand what is happening with factor variables is the **factorMerger** package (see [@factorMerger]).

Here we have Merging Path Plot for a factor variable `district`.

```{r mergingPathPlots, message=FALSE, warning=FALSE, fig.width=12, fig.height=8, fig.cap="Merging Path Plot for `district` variable"}
svd_rf  <- single_variable(explainer_rf, variable = "district", type = "factor")
svd_lm  <- single_variable(explainer_lm, variable = "district", type = "factor")

plot(svd_rf, svd_lm)
```

It looks like these then districts behave like tree groups, the city center (Srodmiescie), districts well communicated with city center (Ochota, Mokotow, Zoliborz) and other districts.

Factor variables are handled very differently by random forest and linear model. But despite of these differences both models result in very similar plots.

