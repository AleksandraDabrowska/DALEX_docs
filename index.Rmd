--- 
title: "DALEX: Descriptive mAchine Learning EXplanations"
author: "Przemys≈Çaw Biecek"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  tufte::tufte_html:
    split_by: chapter
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "Do not trust a black-box model. Unless it explains itself."
--- 

# Introduction

*Machine Learning* models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides.

In many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. 

DALEX is a set of tools that help to understand how complex models are working.

<p><span class="marginnote">Figure 1.1. Workflow of a typical machine learning modeling. <br/>
A) Modeling is a process in which domain knowledge and data are turned into models. <br/>
B) Models are used to generate predictions. <br/>
C) Understanding of model structure may increase our knowledge and in consequence leads to better model. It is hard for black-box models, but DALEX helps here.<br/>
D) Understanding of drivers behind particular model predictions may help to correct wrong decisions and in consequence leads to better model.It is hard for black-box models, but DALEX helps here.
</span>
<img src="images/mp_understanding.png"/></p>


## Motivation

*Machine Learning* is a vague name, there is some *learning* and some *machines*, but what the heck is going on? 

Actually I like the vagueness, because the interpretation of the name evolves over time as the discipline does.

* Few years ago I would extend this name as *machines are learning from human*. In the supervised learning problems, human creates a labeled dataset and machines are tuned to predict labels from data.
* Recently we have more and more examples of *machines that are learning from other machines*. Self playing neural net like AlphaGo Zero [@AlphaGoZero] learns from themself in a blazing speed. Still humans are involved in designing of the learning environment but the labelling turns out to be very expensive or not feasible and we are looking for other ways to learn from partial labels, fuzzy labels, or no labels at all.
* I could imagine that in close future *humans will learn from machines*. Well trained black-boxes may tech us how to be a better in playing go, how to be better in reading PET images, or how to be better in writing of books.

To make this future possible we need tools that extract useful information from black-box models. And as the human supervision over the learning is smaller over time, the black-box understanding is more important. 

DALEX is *the tool* for this.


### Why DALEX?


In recent years there is a lot of works focused in knowledge extraction from complex machine learning models, especially trained with the deep learning techniques. See for example [@Strumbelj], [@nnet_vis], [@magix], [@Zeiler_Fergus_2014].

There are also some R packages that may be used for knowledge extraction from machine learning models. See for example `pdp` [@pdp], `ALEPlot` [@ALEPlot], `randomForestExplainer` [@randomForestExplainer], `xgboostExplainer` [@xgboostExplainer], `live` [@live] and others.

So why do we need yet another R package for this?
There are some unique features of the DALEX package.

* Scope. DALEX is a wrapper over large number of very good tools / model explainers. It offers a wide range of state of the art techniques for model exploration. Some of these techniques are more useful for understanding of model predictions, some are more useful in understanding of structure. 
* Consistency. DALEX offers a consistent grammar across various techniques for model explanation. It's a wrapper that smooths differences across different dependent packages.
* Model agnostic. DALEX explainers are model agnostic, we can use them for linear models, tree ensembles of other structures. So we are not limited to any particular family of black-box models.
* Model comparisons. One can learn a lot from single black-box model, but actually we can learn much more by contrasting models with different structures, like linear models vs. ensembles of trees. All explainers in DALEX by default support model comparisons on various levels.
* Visual consistency. Each DALEX explainer can be plotted with the generic `plot()` function. These visual explanations are based on `ggplot2` [@ggplot2] package, which results in elegant, customizable, consistent graphs.  



Chapter \@ref(architecture) presents the overall architecture of the DALEX package. 
Chapter \@ref(modelUnderstanding) presents explainers that explore global model performance, variable importance of feature effects.
Chapter \@ref(predictionUnderstanding) presents explainers that explore feature attribution for single predictions of validation of the reliability of a model prediction. 


In this document we will focus on three primary use-cases for DALEX explainers.

### To validate

Explainers presented in the section \@ref(modelPerformance) helps to understand model performance and compare different models on the same scale. The model comparison is richer that a one based on a single number and helps to understand model performance along the full range of model predictions.

Explainers presented in the section \@ref(outlierDetection) helps to identify outliers or observations with particularly unusual value.

Explainers presented in the section \@ref(predictionBreakdown) helps to understand which features influence heavily model predictions. 


### To understand

Explainers presented in the section \@ref(featureImportance) helps to understand which variables are the most important in the model. Explainers presented in the section \@ref(predictionBreakdown) helps to understand which features influence single model predictions.  They are useful to understand the key ingredients of the model. 

Explainers presented in the section \@ref(variableResponse) helps to understand how features affect model prediction.

### To improve

Explainers presented in the section \@ref(variableResponse) helps to perform feature engineering based on feature marginal responses.

Explainers presented in the section \@ref(predictionBreakdown) helps to understand which variables affect incorrect model decisions. This is useful to identify and correct biases in the training data.


## Trivia

<span class="marginnote"> 
![](images/dalex01small.jpg)
</span>


[The Daleks](https://en.wikipedia.org/wiki/Dalek) are a fictional extraterrestrial race portrayed in the [Doctor Who](https://en.wikipedia.org/wiki/Doctor_Who) BBC series. Rather dim aliens, known to repeat the phrase *Explain!* very often.

```{r bibliography, include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


