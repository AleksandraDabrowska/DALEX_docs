--- 
title: "DALEX: Descriptive mAchine Learning EXplanations"
author: "Przemys≈Çaw Biecek"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  tufte::tufte_html:
    split_by: chapter
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "Do not trust a black-box model. Unless it explains itself."
--- 

# Introduction

*Machine Learning* (ML) models have lots of applications in classification or regression problems. Due to the increasing computational power of computers and complexity of data sources ML models are more and more sophisticated. Models created with techniques like boosting, bagging of neural networks are described by thousands of parameters. They are obscure, it is hard to trace the link between input variables and model outcomes, in fact they are treated like black boxes.
They are used because of elasticity and high performance, but the lack of interpretability is one of their weakest sides.

In many applications we need to know, understand or prove how the input variables are used in the model. We need to know the impact on particular variables on final model predictions. We need tools that extract useful information from thousands of model parameters.

DALEX is a library with such tools. DALEX helps to understand how complex models are working.
In this document we show two typical use-cases for DALEX: that increase our understanding of a model and that increase our understanding of predictions for particular data points.

<p><span class="marginnote">Figure 1.1. Workflow of a typical machine learning modeling. <br/>
A) Modeling is a process in which domain knowledge and data are turned into models. <br/>
B) Models are used to generate predictions. <br/>
C) Understanding of a model structure may increase our knowledge and in consequence leads to a better model. DALEX helps here.<br/>
D) Understanding of drivers behind particular model predictions may help to correct wrong decisions and in consequence leads to a better model. DALEX helps here.
</span>
<img src="images/mp_understanding.png"/></p>


## Motivation

*Machine Learning* is a vague name. There is some *learning* and some *machines*, but what the heck is going on? What does it really mean? Is it possible that its meaning evolves over time?

* Few years ago I would extend this name to *machines are learning from human*. In the supervised learning problems, human creates a labeled dataset and machines are tuned/trained to predict correct labels from data.
* Recently we have more and more examples of *machines that are learning from other machines*. Self-playing neural nets like AlphaGo Zero [@AlphaGoZero] learns from themself in a blazing speed. Humans are involved in the design of the learning environment but the labeling turns out to be very expensive or not feasible and we are looking for other ways to learn from partial labels, fuzzy labels, or no labels at all.
* I could imagine that in close future *humans will learn from machines*. Well trained black-boxes may tech us how to be a better in playing Go, how to be better in reading PET images (Positron-Emission Tomography images), or how to be better in diagnosing patients.

As the human supervision over the learning is decreasing over time, the black-box understanding is more important. To make this future possible we need tools that extract useful information from black-box models. 

DALEX is *the tool* for this.


### Why DALEX?


In recent years we observe an increasing interest in tools for knowledge extraction from complex machine learning models, see [@Strumbelj], [@nnet_vis], [@magix], [@Zeiler_Fergus_2014].

There are some very useful R packages that may be used for knowledge extraction from R models, see for example `pdp` [@pdp], `ALEPlot` [@ALEPlot], `randomForestExplainer` [@randomForestExplainer], `xgboostExplainer` [@xgboostExplainer], `live` [@live] and others.

Do we need yet another R package to better understand ML models?
I think yes. There are some features available in the DALEX package, which makes it unique.

* Scope. DALEX is a wrapper over large number of very good tools / model explainers. It offers a wide range of state of the art techniques for model exploration. Some of these techniques are more useful for understanding of model predictions, some are more useful in understanding of model structure. 
* Consistency. DALEX offers a consistent grammar across various techniques for model explanation. It's a wrapper that smooth differences across different R packages.
* Model agnostic. DALEX explainers are model agnostic. One can use them for linear models, tree ensembles or other structures. So we are not limited to any particular family of black-box models.
* Model comparisons. One can learn a lot from a single black-box model, but one can learn much more by contrasting models with different structures, like linear models vs. ensembles of trees. All DALEX explainers support model comparisons.
* Visual consistency. Each DALEX explainer can be plotted with the generic `plot()` function. These visual explanations are based on `ggplot2` [@ggplot2] package, which results in elegant, customizable, consistent graphs.  


Chapter \@ref(architecture) presents the overall architecture of the DALEX package. 
Chapter \@ref(modelUnderstanding) presents explainers that explore global model performance, variable importance of feature effects.
Chapter \@ref(predictionUnderstanding) presents explainers that explore feature attribution for single predictions of validation of the reliability of a model prediction. 


In this document we are focused on three primary use-cases for DALEX explainers.

### To validate

Explainers presented in Section \@ref(modelPerformance) help to understand model performance and compare performance of different models.

Explainers presented in Section \@ref(outlierDetection) help to identify outliers or observations with particularly large residuals.

Explainers presented in Section \@ref(predictionBreakdown) help to understand which key features influence model predictions. 

### To understand

Explainers presented in Section \@ref(featureImportance) help to understand which variables are the most important in the model. Explainers presented in Section \@ref(predictionBreakdown) help to understand which features influence single prediction.  They are useful to identify key influencers behind the black-box. 

Explainers presented in Section \@ref(variableResponse) help to understand how features affect model prediction.

### To improve

Explainers presented in Section \@ref(variableResponse) help to perform feature engineering based on model conditional responses.

Explainers presented in Section \@ref(predictionBreakdown) help to understand which variables affect incorrect model decisions. This is useful to identify and correct biases in the training data.


## Trivia

<span class="marginnote"> 
![](images/dalex01small.jpg)
</span>


[The Daleks](https://en.wikipedia.org/wiki/Dalek) are a fictional extraterrestrial race portrayed in the [Doctor Who](https://en.wikipedia.org/wiki/Doctor_Who) BBC series. Rather dim aliens, known to repeat the phrase *Explain!* very often. 
Daleks were engineered. They contain live body in tank-like, robotic shells. Looks like a nice mascot for explanations of Machine Learning models.

```{r bibliography, include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

