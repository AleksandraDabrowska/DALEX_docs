[
["index.html", "DALEX: Descriptive mAchine Learning EXplanations Chapter 1 Introduction 1.1 Notation 1.2 Use case - Human Resources Analytics 1.3 Use case - Wine quality 1.4 Trivia", " DALEX: Descriptive mAchine Learning EXplanations MI2DataLab 2018-01-11 Chapter 1 Introduction Machine Learning models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides. In many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. DALEX is a set of tools that help to understand how complex models are working. 1.1 Notation This book describes explainers for different machine learning models. Some of these explainers are created by different research groups with different applications in mind. To keep the notation consistent: \\(x = (x_1, ..., x_p)\\) stands for a vector of \\(p\\) variables/predictors. \\(y\\) stands for a vector with target variable. In some applications it will be a continuous variable in others it will be a binary variable. \\(n\\) stands for number of observations. \\(f(x, \\theta)\\) stands for a model. We are considering models that are characterized by a set of parameters \\(\\theta\\). In some applications \\(\\theta\\) is a low level space of parameters - nice parametric models, in some applications \\(\\theta\\) may have a very complex structure. \\(\\lambda\\) stands for model meta-parameters which are not being directly optimized (like number of trees, max depth, some penalties etc.). \\(g(x)\\) stands for a function that pre-process variables. In some applications it may be a standardisation or other pre-processing. 1.2 Use case - Human Resources Analytics To ilustrate applications of DALEX to binary classification problems we will use a dataset from Kaggle competition Human Resources Analytics. This dataset is avaliable in the breakDown package (P. Biecek 2017). library(&quot;breakDown&quot;) head(HR_data) (#tab:hr_data)HR_data dataset from Kaggle competition Human Resources Analytics satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years sales salary 0.38 0.53 2 157 3 0 1 0 sales low 0.80 0.86 5 262 6 0 1 0 sales medium 0.11 0.88 7 272 4 0 1 0 sales medium 0.72 0.87 5 223 5 0 1 0 sales low 0.37 0.52 2 159 3 0 1 0 sales low 0.41 0.50 2 153 3 0 1 0 sales low 1.2.1 Logistic regression In the following chapters to present explainers for logistic regression models we will use HR_glm_model. HR_glm_model &lt;- glm(left~., data = HR_data, family = &quot;binomial&quot;) summary(HR_glm_model) ## ## Call: ## glm(formula = left ~ ., family = &quot;binomial&quot;, data = HR_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2248 -0.6645 -0.4026 -0.1177 3.0688 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.4762862 0.1938373 -7.616 2.61e-14 *** ## satisfaction_level -4.1356889 0.0980538 -42.178 &lt; 2e-16 *** ## last_evaluation 0.7309032 0.1491787 4.900 9.61e-07 *** ## number_project -0.3150787 0.0213248 -14.775 &lt; 2e-16 *** ## average_montly_hours 0.0044603 0.0005161 8.643 &lt; 2e-16 *** ## time_spend_company 0.2677537 0.0155736 17.193 &lt; 2e-16 *** ## Work_accident -1.5298283 0.0895473 -17.084 &lt; 2e-16 *** ## promotion_last_5years -1.4301364 0.2574958 -5.554 2.79e-08 *** ## saleshr 0.2323779 0.1313084 1.770 0.07678 . ## salesIT -0.1807179 0.1221276 -1.480 0.13894 ## salesmanagement -0.4484236 0.1598254 -2.806 0.00502 ** ## salesmarketing -0.0120882 0.1319304 -0.092 0.92700 ## salesproduct_mng -0.1532529 0.1301538 -1.177 0.23901 ## salesRandD -0.5823659 0.1448848 -4.020 5.83e-05 *** ## salessales -0.0387859 0.1024006 -0.379 0.70486 ## salessupport 0.0500251 0.1092834 0.458 0.64713 ## salestechnical 0.0701464 0.1065379 0.658 0.51027 ## salarylow 1.9440627 0.1286272 15.114 &lt; 2e-16 *** ## salarymedium 1.4132244 0.1293534 10.925 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16465 on 14998 degrees of freedom ## Residual deviance: 12850 on 14980 degrees of freedom ## AIC: 12888 ## ## Number of Fisher Scoring iterations: 5 Models used in this doccumentation are accessible via archivist package. To download the HR_glm_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/8fe19a108faf3ddfcabc3de3a0693234&quot;) 1.2.2 Random forest In the following chapters to present explainers for random forest models we will use HR_fr_model. library(&quot;randomForest&quot;) set.seed(1313) HR_data$left &lt;- factor(HR_data$left) HR_rf_model &lt;- randomForest(left~., data = HR_data, ntree = 100) HR_rf_model ## ## Call: ## randomForest(formula = left ~ ., data = HR_data, ntree = 100) ## Type of random forest: classification ## Number of trees: 100 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 0.75% ## Confusion matrix: ## 0 1 class.error ## 0 11407 21 0.001837592 ## 1 91 3480 0.025483058 Models used in this doccumentation are accessible via archivist package. To download the HR_rf_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/419d550a92fab6a5f28650130991e2cd&quot;) 1.3 Use case - Wine quality To ilustrate applications of DALEX to regression problems we will use a Wine quality dataset from Kaggle competition UC Irvine Machine Learning Repository. url &lt;- &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&#39; wine &lt;- read.table(url, header = TRUE, sep = &quot;;&quot;) head(wine) Table 1.1: Wine quality dataset from UC Irvine Machine Learning Repository fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality 7.0 0.27 0.36 20.7 0.045 45 170 1.0010 3.00 0.45 8.8 6 6.3 0.30 0.34 1.6 0.049 14 132 0.9940 3.30 0.49 9.5 6 8.1 0.28 0.40 6.9 0.050 30 97 0.9951 3.26 0.44 10.1 6 7.2 0.23 0.32 8.5 0.058 47 186 0.9956 3.19 0.40 9.9 6 7.2 0.23 0.32 8.5 0.058 47 186 0.9956 3.19 0.40 9.9 6 8.1 0.28 0.40 6.9 0.050 30 97 0.9951 3.26 0.44 10.1 6 1.3.1 Linear regression In the following chapters to present explainers for gaussian regression models we will use wine_lm_model. wine_lm_model &lt;- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, data = wine) Models used in this doccumentation are accessible via archivist package. To download the wine_lm_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/b99a3d58016e2677221019652cff047f&quot;) 1.4 Trivia The Daleks are a fictional extraterrestrial race portrayed in the Doctor Who BBC series. Rather dim aliens, known to repeat the phrase Explain! very often. References "],
["global-structure.html", "Chapter 2 Global structure 2.1 Forest plots 2.2 Variable Importance Plot 2.3 ctree 2.4 RandomForestExplainer", " Chapter 2 Global structure Explainers presented in this chapter are designed to better understand the global structure of a black box. Which variables are the most important? How do they influence the final result of a model? 2.1 Forest plots Forest plots were initially used in the meta analysis to visualise effects in different studies. But now they are frequently used to present summary characteristics for models with linear structure like these created with lm or glm functions. There are various implementations of forest plots in R. Below we present examples for a glm model. library(&quot;breakDown&quot;) HR_glm_model &lt;- glm(left~., data = HR_data, family = &quot;binomial&quot;) #HR_glm_model &lt;- archivist::aread(&quot;pbiecek/DALEX/arepo/8fe19a108faf3ddfcabc3de3a0693234&quot;) In the package forestmodel (see (Kennedy 2017)) one can use forest_model() function to draw a forest plot. This package is based on the broom package (see (Robinson 2017)) and this is why it handles a large variety of different regression models. An example for glm. library(&quot;forestmodel&quot;) forest_model(HR_glm_model) Figure 2.1: Forest plot created with forestmodel package In the package sjPlot (see (LÃ¼decke 2017)) one can use sjp.*() to visualise structure of a * model or a wrapper plot_model(). Here is an example for glm model. library(&quot;sjPlot&quot;) plot_model(HR_glm_model, type = &quot;est&quot;, sort.est = TRUE) Figure 2.2: Forest plot created with sjPlot package Note! The forestmodel package handles factor variables in a better way while the plots from sjPlot are easier to read. 2.2 Variable Importance Plot 2.3 ctree 2.4 RandomForestExplainer References "],
["conditional-structure.html", "Chapter 3 Conditional structure 3.1 Partial Dependence Plot 3.2 Individual Conditional Expectation Plot 3.3 Accumulated Local Effects Plot 3.4 Mering Path Plot", " Chapter 3 Conditional structure The dimension of input \\(x\\) for black box models is usually high (large \\(p\\)). But in many cases small number of variables play important role in the model OR there are some reasons to believe that variables work in an additive fashion/low-level interactions in the model. In such cases one may be interesting in verification how the conditional response for a selected interesting variable/variables looks like. Methods presented below help to understand the conditional structure of a model. 3.1 Partial Dependence Plot Partial Dependence Plots (see pdp package (Greenwell 2017)) for a black box \\(f(x; \\theta)\\) calculates the expected output given a selected variable. \\[ p_i(x_i) = E_{x_{-i}}[ f(x^i, x^{-i}; \\theta) ] \\] Of course this expectation cannot be calculated directly as we do not know fully the \\(f()\\) neither the distribution of \\(x_{-i}\\). This value is estimated by \\[ \\hat p_i(x_i) = \\frac{1}{n} \\sum_{j=1}^{n} f(x^i_j, x_j^{-i}, \\hat \\theta) \\] Letâs see an example for the model HR_rf_model. Below we are using pdp::partial function to calculate pdp curve for variable satisfaction_level. Then the curve is plotted with plotPartial (based on lattice) or autoplot (based on ggplot2). library(&quot;pdp&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) part &lt;- partial(HR_rf_model, &quot;satisfaction_level&quot;) plotPartial(part) library(&quot;ggplot2&quot;) autoplot(part) 3.2 Individual Conditional Expectation Plot Individual Conditional Expectations (ICE) may be considered as an extension of the PDP curves (see ICEbox package (Goldstein et al. 2015)). Instead of plotting expected value over all observations, for ICE we are plotting individual conditional model responses. Average of ICE curves results in PDP curve. An ICE curve for observation \\(k\\) over variable \\(i\\) may be defined as \\[ ice_k(x_i) = f(x^i, x_k^{-i}; \\theta) \\] ICE curves can be plotted with pdp package. Note that curves may be cantered in a given point, this helps in identification of possible interactions. library(&quot;pdp&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) library(&quot;ggplot2&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) part_rf_satisfaction &lt;- partial(HR_rf_model, &quot;satisfaction_level&quot;) part_rf_satisfaction &lt;- partial(HR_rf_model, pred.var = &quot;satisfaction_level&quot;, ice = TRUE) plotPartial(part_rf_satisfaction, rug = TRUE, train = HR_data, alpha = 0.2) autoplot(part_rf_satisfaction, center = TRUE, alpha = 0.2, rug = TRUE, train = HR_data) Or with the ICEbox package. library(&quot;ICEbox&quot;) part_rf_satisfaction = ice(object = HR_rf_model, X = HR_data, y = HR_data$satisfaction_level, predictor = &quot;satisfaction_level&quot;, frac_to_build = .1) ## ............................................................................................ plot(part_rf_satisfaction) As ICE curves are useful tool for identification of interactions, these individual curves may be clustered with the clusterICE function. clusterICE(part_rf_satisfaction, nClusters = 3, plot_legend = TRUE, center = TRUE) 3.3 Accumulated Local Effects Plot As it is presented in the chapter @(pdpchapter), the Partial Dependence Plot presents the expected model response with respect to marginal distribution of \\(x_{-i}\\). In some cases, e.g. when repressors are highly correlated, expectation over the marginal distribution may lead to biases/poorly extrapolated model responses. Especially in area far from the training set (see (Apley 2017) for more details). Accumulated local effects (ALE) plots (see ALEPlot package (Apley 2017)) solves this problem by using conditional distribution \\(x_{-i}|x_i = x_i^*\\). This leads to more stable and reliable estimates (at least when the predictors are highly correlated). Let see an example for ALE plots. library(&quot;ALEPlot&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) HR_data_small &lt;- HR_data[,c(&quot;satisfaction_level&quot;, &quot;last_evaluation&quot;, &quot;number_project&quot;, &quot;average_montly_hours&quot;, &quot;time_spend_company&quot;, &quot;Work_accident&quot;, &quot;promotion_last_5years&quot;, &quot;sales&quot;, &quot;salary&quot;)] Estimation of main effects for satisfaction_level is similar to the PDP curves ale1d &lt;- ALEPlot(X = HR_data_small, X.model = HR_rf_model, pred.fun = function(X.model, newdata) { predict(X.model, newdata) }, J = 1) And here we have predictions for satisfaction_level and last_evaluation. ale2d &lt;- ALEPlot(X = HR_data_small, X.model = HR_rf_model, pred.fun = function(X.model, newdata) { predict(X.model, newdata) }, J = c(1,2)) 3.4 Mering Path Plot The package ICEbox is not working for factor variables while the pdp package returns plots that are hard to interpret. An interesting tool that helps to understand what is happening with factor variables is the factorMerger package (see (Sitko and Biecek 2017)). Here we have Merging Path Plot for a factor variable sales. library(&quot;factorMerger&quot;) path &lt;- mergeFactors(HR_data$left, HR_data$sales, method = &quot;fast-adaptive&quot;, family = &quot;binomial&quot;, abbreviate = FALSE) plot(path, panel = &quot;all&quot;) Note that you can use the factorMerger package to understand predictions calculated with a black-box model. The random forest model HR_rf_model returns continuous response. But the factorMerger works for such variables as well. In the top right panel one mey see the distribution of predictions for the selected group. HR_data$left_predicted &lt;- predict(HR_rf_model) path &lt;- mergeFactors(HR_data$left_predicted, HR_data$sales, method = &quot;fast-adaptive&quot;, abbreviate = FALSE) plot(path, panel = &quot;all&quot;, responsePanel = &quot;boxplot&quot;, nodesSpacing = &quot;effects&quot;) References "],
["local-structure.html", "Chapter 4 Local structure 4.1 Local Interpretable (Model-agnostic) Visual Explanations 4.2 breakDown", " Chapter 4 Local structure Explainers presented in this chapter are designed to better understand the local structure of a black box in a single point. Example applications: explanations for predictions. Can be used to validate if a specific prediction is not accidental, is it based on variables important in the domain. examination of curvature around a specific point (single observation). Can be used to determine the strength of influence onto a final model. Is it an outlier? There are more interesting applications. Find out some of them in the Why Should I Trust You? article (Ribeiro, Singh, and Guestrin 2016). 4.1 Local Interpretable (Model-agnostic) Visual Explanations The live package (see (Staniak and Biecek 2017)) may be seen as an extension of the lime method (see (Ribeiro, Singh, and Guestrin 2016)). It is based on mlr general framework for training of machine learning models (see more (Bischl et al. 2016)). Letâs see an example. We will use the HR_rf_model trained with the randomForest package on Human Resources Analytics data. Around a selected point we will fit a linear model. library(&quot;live&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) HR_data$left &lt;- as.numeric(as.character(HR_data$left)) HR_rf_model &lt;- randomForest(left~., data = HR_data, ntree=100) similar &lt;- sample_locally(data = HR_data, explained_instance = HR_data[1,], explained_var = &quot;left&quot;, size = 2000) similar &lt;- add_predictions(HR_data, similar, HR_rf_model) trained &lt;- fit_explanation( live_object = similar, white_box = &quot;regr.lm&quot;, selection = FALSE) Fitted model may be plotted with waterfall plot â¦ plot_explanation(trained, &quot;waterfallplot&quot;, explained_instance = HR_data[1,]) â¦ or forest plot â¦ plot_explanation(trained, &quot;forestplot&quot;, explained_instance = HR_data[1,]) For more details consult the following vignette. 4.2 breakDown (P. Biecek 2017) References "],
["model-performance.html", "Chapter 5 Model performance 5.1 ROC 5.2 Lift curve", " Chapter 5 Model performance 5.1 ROC 5.2 Lift curve "],
["goodness-of-fit-model-diagnostic.html", "Chapter 6 Goodness of fit / Model diagnostic 6.1 halfnorm plot", " Chapter 6 Goodness of fit / Model diagnostic 6.1 halfnorm plot "],
["reproducibility.html", "Chapter 7 Reproducibility", " Chapter 7 Reproducibility Packages are changing quite fast, especially in actively developed areas. Below you will find list of packages that were installed on my computer when I was preparing this documentation. It is likely that some of described packages will change names of functions or arguments or structure of results. Use the version listed below to reproduce results form this book. Note, that results, models and plots created in are were recorded with the archivist package (P. Biecek and Kosinski 2017). Use archivist links to retrieve their binary copies directly to your R console. devtools::session_info() ## setting value ## version R version 3.4.3 (2017-11-30) ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz Europe/Warsaw ## date 2018-01-11 ## ## package * version date ## abind 1.4-5 2016-07-21 ## agricolae 1.2-8 2017-09-12 ## ALEPlot * 1.0 2017-11-13 ## AlgDesign 1.1-7.3 2014-10-15 ## arm 1.9-3 2016-11-27 ## assertthat 0.2.0 2017-04-11 ## backports 1.1.1 2017-09-25 ## base * 3.4.3 2017-12-07 ## bayesplot 1.4.0 2017-09-12 ## BBmisc 1.11 2017-03-10 ## bindr 0.1 2016-11-13 ## bindrcpp * 0.2 2017-06-17 ## blme 1.0-4 2015-06-14 ## bookdown 0.5 2017-08-20 ## boot 1.3-20 2017-08-06 ## breakDown * 0.1.2 2018-01-02 ## broom 0.4.2 2017-02-13 ## carData 3.0-0 2017-08-28 ## checkmate 1.8.4 2017-09-25 ## cli 1.0.0 2017-11-05 ## cluster 2.0.6 2017-03-10 ## coda 0.19-1 2016-12-08 ## codetools 0.2-15 2016-10-05 ## coin 1.2-1 2017-07-17 ## colorspace 1.3-2 2016-12-14 ## combinat 0.0-8 2012-10-29 ## compiler 3.4.3 2017-12-07 ## cowplot 0.9.1 2017-11-16 ## crayon 1.3.4 2017-09-16 ## data.table 1.10.4-3 2017-10-27 ## datasets * 3.4.3 2017-12-07 ## deldir 0.1-14 2017-04-22 ## devtools 1.13.3 2017-08-02 ## digest 0.6.13 2017-12-14 ## dplyr * 0.7.4 2017-09-28 ## DT 0.1 2015-06-09 ## effects 4.0-0 2017-09-15 ## evaluate 0.10.1 2017-06-24 ## expm 0.999-2 2017-03-29 ## factorMerger * 0.3.4 2018-01-05 ## forcats 0.2.0 2017-01-23 ## foreign 0.8-69 2017-06-22 ## forestmodel * 0.4.3 2017-04-16 ## forestplot 1.7.2 2017-09-16 ## gdata 2.17.0 2015-07-04 ## ggeffects 0.3.0 2017-11-27 ## ggplot2 * 2.2.1 2016-12-30 ## ggpubr 0.1.6 2017-11-14 ## glmmTMB 0.2.0 2017-12-11 ## glue 1.1.1 2017-06-21 ## gmodels 2.16.2 2015-07-22 ## graphics * 3.4.3 2017-12-07 ## grDevices * 3.4.3 2017-12-07 ## grid 3.4.3 2017-12-07 ## gridExtra 2.3 2017-09-09 ## gtable 0.2.0 2016-02-26 ## gtools 3.5.0 2015-05-29 ## haven 1.1.0 2017-07-09 ## highr 0.6 2016-05-09 ## htmltools 0.3.6 2017-04-28 ## htmlwidgets 0.9 2017-07-10 ## httpuv 1.3.5 2017-07-04 ## ICEbox * 1.1.2 2017-07-13 ## klaR 0.6-12 2014-08-06 ## knitr 1.18 2017-12-27 ## labeling 0.3 2014-08-23 ## lattice 0.20-35 2017-03-25 ## lazyeval 0.2.0 2016-06-12 ## LearnBayes 2.15 2014-05-29 ## live * 0.2.0 2017-12-07 ## lme4 1.1-15 2017-12-21 ## lmtest 0.9-34 2015-06-06 ## magrittr 1.5 2014-11-22 ## MASS 7.3-47 2017-02-26 ## Matrix 1.2-12 2017-11-15 ## memoise 1.1.0 2017-04-21 ## merTools 0.3.0 2016-12-12 ## methods * 3.4.3 2017-12-07 ## mime 0.5 2016-07-07 ## minqa 1.2.4 2014-10-09 ## mlr * 2.11 2017-03-15 ## mnormt 1.5-5 2016-10-15 ## modelr 0.1.1 2017-07-24 ## modeltools 0.2-21 2013-09-02 ## multcomp 1.4-7 2017-09-07 ## munsell 0.4.3 2016-02-13 ## mvtnorm 1.0-6 2017-03-02 ## nlme 3.1-131 2017-02-06 ## nloptr 1.0.4 2014-08-04 ## nnet 7.3-12 2016-02-02 ## parallel 3.4.3 2017-12-07 ## parallelMap 1.3 2015-06-10 ## ParamHelpers * 1.10 2017-01-05 ## pdp * 0.6.0 2017-07-20 ## pkgconfig 2.0.1 2017-03-21 ## plyr 1.8.4 2016-06-08 ## prediction 0.2.0 2017-04-19 ## proxy 0.4-20 2017-12-12 ## psych 1.7.3.21 2017-03-22 ## purrr 0.2.4 2017-10-18 ## pwr 1.2-1 2017-03-25 ## R6 2.2.2 2017-06-17 ## randomForest * 4.6-12 2015-10-07 ## RColorBrewer 1.1-2 2014-12-07 ## Rcpp 0.12.14 2017-11-23 ## reshape2 1.4.3 2017-12-11 ## rlang 0.1.6 2017-12-21 ## rmarkdown 1.8 2017-11-17 ## rprojroot 1.2 2017-01-16 ## rstudioapi 0.7 2017-09-07 ## sandwich 2.4-0 2017-07-26 ## scales 0.5.0 2017-08-24 ## sfsmisc * 1.1-1 2017-06-08 ## shiny 1.0.5 2017-08-23 ## sjlabelled 1.0.5 2017-11-09 ## sjmisc 2.6.3 2017-11-28 ## sjPlot * 2.4.0 2017-10-19 ## sjstats 0.13.0 2017-11-23 ## snakecase 0.5.1 2017-09-20 ## sp 1.2-5 2017-06-29 ## spData 0.2.6.4 2017-11-12 ## spdep 0.7-4 2017-11-22 ## splines 3.4.3 2017-12-07 ## stats * 3.4.3 2017-12-07 ## stats4 3.4.3 2017-12-07 ## stringdist 0.9.4.1 2016-01-02 ## stringi 1.1.6 2017-11-17 ## stringr 1.2.0 2017-02-18 ## survey 3.30-3 2014-08-15 ## survival 2.41-3 2017-04-04 ## TH.data 1.0-8 2017-01-23 ## tibble 1.3.4 2017-08-22 ## tidyr 0.7.2 2017-10-16 ## tidyselect 0.2.2 2017-10-10 ## TMB 1.7.12 2017-12-11 ## tools 3.4.3 2017-12-07 ## utils * 3.4.3 2017-12-07 ## withr 2.1.0 2017-11-01 ## xtable 1.8-2 2016-02-05 ## yaImpute 1.0-29 2017-12-10 ## yaml 2.1.16 2017-12-12 ## zoo 1.8-0 2017-04-12 ## source ## cran (@1.4-5) ## cran (@1.2-8) ## CRAN (R 3.4.2) ## CRAN (R 3.2.0) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## CRAN (R 3.4.2) ## local ## CRAN (R 3.4.3) ## cran (@1.11) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## CRAN (R 3.4.1) ## CRAN (R 3.4.3) ## local (pbiecek/breakDown@NA) ## CRAN (R 3.4.0) ## CRAN (R 3.4.1) ## cran (@1.8.4) ## cran (@1.0.0) ## CRAN (R 3.4.3) ## cran (@0.19-1) ## CRAN (R 3.4.3) ## cran (@1.2-1) ## CRAN (R 3.4.0) ## CRAN (R 3.1.0) ## local ## cran (@0.9.1) ## CRAN (R 3.4.1) ## CRAN (R 3.4.2) ## local ## cran (@0.1-14) ## CRAN (R 3.4.1) ## cran (@0.6.13) ## CRAN (R 3.4.2) ## CRAN (R 3.2.0) ## CRAN (R 3.4.1) ## CRAN (R 3.4.1) ## cran (@0.999-2) ## Github (MI2DataLab/factorMerger@0ccb928) ## CRAN (R 3.4.0) ## CRAN (R 3.4.3) ## CRAN (R 3.4.0) ## cran (@1.7.2) ## CRAN (R 3.2.0) ## CRAN (R 3.4.3) ## CRAN (R 3.4.0) ## cran (@0.1.6) ## CRAN (R 3.4.3) ## CRAN (R 3.4.1) ## CRAN (R 3.4.0) ## local ## local ## local ## CRAN (R 3.4.1) ## CRAN (R 3.2.3) ## CRAN (R 3.2.0) ## CRAN (R 3.4.1) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## CRAN (R 3.4.1) ## CRAN (R 3.4.1) ## CRAN (R 3.4.1) ## CRAN (R 3.2.0) ## cran (@1.18) ## CRAN (R 3.2.2) ## CRAN (R 3.4.3) ## CRAN (R 3.4.0) ## CRAN (R 3.2.0) ## Github (MI2DataLab/live@ec7e71d) ## cran (@1.1-15) ## CRAN (R 3.2.0) ## CRAN (R 3.2.2) ## CRAN (R 3.4.3) ## CRAN (R 3.4.2) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## local ## CRAN (R 3.4.0) ## CRAN (R 3.1.1) ## cran (@2.11) ## CRAN (R 3.4.0) ## CRAN (R 3.4.1) ## CRAN (R 3.1.2) ## cran (@1.4-7) ## CRAN (R 3.2.3) ## CRAN (R 3.4.0) ## CRAN (R 3.4.3) ## CRAN (R 3.2.2) ## CRAN (R 3.4.3) ## local ## cran (@1.3) ## cran (@1.10) ## CRAN (R 3.4.1) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## cran (@0.4-20) ## CRAN (R 3.4.0) ## cran (@0.2.4) ## CRAN (R 3.4.0) ## CRAN (R 3.4.0) ## CRAN (R 3.2.0) ## CRAN (R 3.2.2) ## cran (@0.12.14) ## cran (@1.4.3) ## CRAN (R 3.4.3) ## cran (@1.8) ## CRAN (R 3.4.0) ## CRAN (R 3.4.1) ## cran (@2.4-0) ## CRAN (R 3.4.1) ## CRAN (R 3.4.0) ## CRAN (R 3.4.1) ## CRAN (R 3.4.2) ## CRAN (R 3.4.3) ## CRAN (R 3.4.2) ## CRAN (R 3.4.3) ## CRAN (R 3.4.2) ## cran (@1.2-5) ## cran (@0.2.6.4) ## cran (@0.7-4) ## local ## local ## local ## CRAN (R 3.2.3) ## cran (@1.1.6) ## CRAN (R 3.4.0) ## CRAN (R 3.1.2) ## CRAN (R 3.4.3) ## cran (@1.0-8) ## CRAN (R 3.4.1) ## cran (@0.7.2) ## cran (@0.2.2) ## CRAN (R 3.4.3) ## local ## local ## cran (@2.1.0) ## CRAN (R 3.2.3) ## CRAN (R 3.4.3) ## cran (@2.1.16) ## CRAN (R 3.4.0) "]
]
