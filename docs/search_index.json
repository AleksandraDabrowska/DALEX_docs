[
["index.html", "DALEX: Descriptive mAchine Learning EXplanations Chapter 1 Basics 1.1 Introduction 1.2 Notation 1.3 Use case - Human Resources Analytics 1.4 Use case - Wine quality 1.5 Reproducibility 1.6 Trivia", " DALEX: Descriptive mAchine Learning EXplanations MI2DataLab 2018-01-04 Chapter 1 Basics 1 Basics 2 Global structure 3 Conditional structure 4 Local structure 5 Model performance 1.1 Introduction 1.1 Introduction 1.2 Notation 1.3 Use case: Human Resource Analytics 1.4 Use case: Wine quality 1.5 Reproducibility 1.6 Trivia Machine Learning models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides. In many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. DALEX is a set of tools that help to understand how complex models are working. 1.2 Notation This book describes explainers for different machine learning models. Some of these explainers are created by different research groups with different applications in mind. To keep the notation consistent: \\(x = (x_1, ..., x_p)\\) stands for a vector of \\(p\\) variables/predictors. \\(y\\) stands for a vector with target variable. In some applications it will be a continuous variable in others it will be a binary variable. \\(n\\) stands for number of observations. \\(f(x, \\theta)\\) stands for a model. We are considering models that are characterized by a set of parameters \\(\\theta\\). In some applications \\(\\theta\\) is a low level space of parameters - nice parametric models, in some applications \\(\\theta\\) may have a very complex structure. \\(\\lambda\\) stands for model meta-parameters which are not being directly optimized (like number of trees, max depth, some penalties etc.). \\(g(x)\\) stands for a function that pre-process variables. In some applications it may be a standardisation or other pre-processing. 1.3 Use case - Human Resources Analytics To ilustrate applications of DALEX to binary classification problems we will use a dataset from Kaggle competition Human Resources Analytics. This dataset is avaliable in the breakDown package (P. Biecek 2017). library(&quot;breakDown&quot;) head(HR_data) (#tab:hr_data)HR_data dataset from Kaggle competition Human Resources Analytics satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years sales salary 0.38 0.53 2 157 3 0 1 0 sales low 0.80 0.86 5 262 6 0 1 0 sales medium 0.11 0.88 7 272 4 0 1 0 sales medium 0.72 0.87 5 223 5 0 1 0 sales low 0.37 0.52 2 159 3 0 1 0 sales low 0.41 0.50 2 153 3 0 1 0 sales low 1.3.1 Logistic regression In the following chapters to present explainers for logistic regression models we will use HR_glm_model. HR_glm_model &lt;- glm(left~., data = HR_data, family = &quot;binomial&quot;) summary(HR_glm_model) ## ## Call: ## glm(formula = left ~ ., family = &quot;binomial&quot;, data = HR_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2248 -0.6645 -0.4026 -0.1177 3.0688 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.4762862 0.1938373 -7.616 2.61e-14 *** ## satisfaction_level -4.1356889 0.0980538 -42.178 &lt; 2e-16 *** ## last_evaluation 0.7309032 0.1491787 4.900 9.61e-07 *** ## number_project -0.3150787 0.0213248 -14.775 &lt; 2e-16 *** ## average_montly_hours 0.0044603 0.0005161 8.643 &lt; 2e-16 *** ## time_spend_company 0.2677537 0.0155736 17.193 &lt; 2e-16 *** ## Work_accident -1.5298283 0.0895473 -17.084 &lt; 2e-16 *** ## promotion_last_5years -1.4301364 0.2574958 -5.554 2.79e-08 *** ## saleshr 0.2323779 0.1313084 1.770 0.07678 . ## salesIT -0.1807179 0.1221276 -1.480 0.13894 ## salesmanagement -0.4484236 0.1598254 -2.806 0.00502 ** ## salesmarketing -0.0120882 0.1319304 -0.092 0.92700 ## salesproduct_mng -0.1532529 0.1301538 -1.177 0.23901 ## salesRandD -0.5823659 0.1448848 -4.020 5.83e-05 *** ## salessales -0.0387859 0.1024006 -0.379 0.70486 ## salessupport 0.0500251 0.1092834 0.458 0.64713 ## salestechnical 0.0701464 0.1065379 0.658 0.51027 ## salarylow 1.9440627 0.1286272 15.114 &lt; 2e-16 *** ## salarymedium 1.4132244 0.1293534 10.925 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16465 on 14998 degrees of freedom ## Residual deviance: 12850 on 14980 degrees of freedom ## AIC: 12888 ## ## Number of Fisher Scoring iterations: 5 Models used in this doccumentation are accessible via archivist package. To download the HR_glm_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/8fe19a108faf3ddfcabc3de3a0693234&quot;) 1.3.2 Random forest In the following chapters to present explainers for random forest models we will use HR_fr_model. library(&quot;randomForest&quot;) set.seed(1313) HR_data$left &lt;- factor(HR_data$left) HR_rf_model &lt;- randomForest(left~., data = HR_data, ntree = 100) HR_rf_model ## ## Call: ## randomForest(formula = left ~ ., data = HR_data, ntree = 100) ## Type of random forest: classification ## Number of trees: 100 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 0.75% ## Confusion matrix: ## 0 1 class.error ## 0 11407 21 0.001837592 ## 1 91 3480 0.025483058 Models used in this doccumentation are accessible via archivist package. To download the HR_rf_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/419d550a92fab6a5f28650130991e2cd&quot;) 1.4 Use case - Wine quality To ilustrate applications of DALEX to regression problems we will use a Wine quality dataset from Kaggle competition UC Irvine Machine Learning Repository. url &lt;- &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&#39; wine &lt;- read.table(url, header = TRUE, sep = &quot;;&quot;) head(wine) Table 1.1: Wine quality dataset from UC Irvine Machine Learning Repository fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality 7.0 0.27 0.36 20.7 0.045 45 170 1.0010 3.00 0.45 8.8 6 6.3 0.30 0.34 1.6 0.049 14 132 0.9940 3.30 0.49 9.5 6 8.1 0.28 0.40 6.9 0.050 30 97 0.9951 3.26 0.44 10.1 6 7.2 0.23 0.32 8.5 0.058 47 186 0.9956 3.19 0.40 9.9 6 7.2 0.23 0.32 8.5 0.058 47 186 0.9956 3.19 0.40 9.9 6 8.1 0.28 0.40 6.9 0.050 30 97 0.9951 3.26 0.44 10.1 6 1.4.1 Linear regression In the following chapters to present explainers for gaussian regression models we will use wine_lm_model. wine_lm_model &lt;- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, data = wine) Models used in this doccumentation are accessible via archivist package. To download the wine_lm_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/b99a3d58016e2677221019652cff047f&quot;) 1.5 Reproducibility Packages are changing quite fast, especially in actively developed areas. Below you will find list of packages that were installed on my computer when I was preparing this documentation. It is likely that some of described packages will change names of functions or arguments or structure of results. Use the version listed below to reproduce results form this book. Note, that results, models and plots created in are were recorded with the archivist package (P. Biecek and Kosinski 2017). Use archivist links to retrieve their binary copies directly to your R console. devtools::session_info() ## setting value ## version R version 3.4.2 (2017-09-28) ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz Europe/Warsaw ## date 2018-01-04 ## ## package * version date source ## backports 1.1.1 2017-09-25 CRAN (R 3.4.2) ## base * 3.4.2 2017-10-04 local ## bookdown 0.5 2017-08-20 CRAN (R 3.4.1) ## breakDown * 0.1.2 2018-01-02 local (pbiecek/breakDown@NA) ## colorspace 1.3-2 2016-12-14 CRAN (R 3.4.0) ## compiler 3.4.2 2017-10-04 local ## datasets * 3.4.2 2017-10-04 local ## devtools 1.13.3 2017-08-02 CRAN (R 3.4.1) ## digest 0.6.12 2017-01-27 CRAN (R 3.4.0) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.4.1) ## ggplot2 2.2.1 2016-12-30 CRAN (R 3.4.0) ## graphics * 3.4.2 2017-10-04 local ## grDevices * 3.4.2 2017-10-04 local ## grid 3.4.2 2017-10-04 local ## gtable 0.2.0 2016-02-26 CRAN (R 3.2.3) ## highr 0.6 2016-05-09 CRAN (R 3.4.0) ## htmltools 0.3.6 2017-04-28 CRAN (R 3.4.0) ## knitr 1.17 2017-08-10 CRAN (R 3.4.1) ## lazyeval 0.2.0 2016-06-12 CRAN (R 3.4.0) ## magrittr 1.5 2014-11-22 CRAN (R 3.2.2) ## memoise 1.1.0 2017-04-21 CRAN (R 3.4.0) ## methods * 3.4.2 2017-10-04 local ## munsell 0.4.3 2016-02-13 CRAN (R 3.2.3) ## plyr 1.8.4 2016-06-08 CRAN (R 3.4.0) ## randomForest * 4.6-12 2015-10-07 CRAN (R 3.2.0) ## Rcpp 0.12.14 2017-11-23 cran (@0.12.14) ## rlang 0.1.4.9000 2017-11-30 Github (tidyverse/rlang@5c3e30f) ## rmarkdown 1.8 2017-11-17 cran (@1.8) ## rprojroot 1.2 2017-01-16 CRAN (R 3.4.0) ## rstudioapi 0.7 2017-09-07 CRAN (R 3.4.1) ## scales 0.5.0 2017-08-24 CRAN (R 3.4.1) ## stats * 3.4.2 2017-10-04 local ## stringi 1.1.6 2017-11-17 cran (@1.1.6) ## stringr 1.2.0 2017-02-18 CRAN (R 3.4.0) ## tibble 1.3.4 2017-08-22 CRAN (R 3.4.1) ## tools 3.4.2 2017-10-04 local ## utils * 3.4.2 2017-10-04 local ## withr 2.1.0 2017-11-01 cran (@2.1.0) ## yaml 2.1.14 2016-11-12 CRAN (R 3.4.0) 1.6 Trivia The Daleks are a fictional extraterrestrial race portrayed in the Doctor Who BBC series. Rather dim aliens, known to repeat the phrase Explain! very often. References "],
["global-structure.html", "Chapter 2 Global structure 2.1 Forest plots", " Chapter 2 Global structure 2.1 Forest plots Forest plots were initially used in the meta analysis to visualise effects in different studies. But now they are frequently used to present summary characteristics for models with linear structure like these created with lm or glm functions. There are various implementations of forest plots in R. Below we present examples for a glm model. library(&quot;breakDown&quot;) HR_glm_model &lt;- glm(left~., data = HR_data, family = &quot;binomial&quot;) #HR_glm_model &lt;- archivist::aread(&quot;pbiecek/DALEX/arepo/8fe19a108faf3ddfcabc3de3a0693234&quot;) In the package forestmodel (see (Kennedy 2017)) one can use forest_model() function to draw a forest plot. This package is based on the broom package (see (Robinson 2017)) and this is why it handles a large variety of different regression models. An example for glm. library(&quot;forestmodel&quot;) forest_model(HR_glm_model) Figure 2.1: Forest plot created with forestmodel package In the package sjPlot (see (Lüdecke 2017)) one can use sjp.*() to visualise structure of a * model or a wrapper plot_model(). Here is an example for glm model. library(&quot;sjPlot&quot;) plot_model(HR_glm_model, type = &quot;est&quot;, sort.est = TRUE) Figure 2.2: Forest plot created with sjPlot package Note! The forestmodel package handles factor variables in a better way while the plots from sjPlot are easier to read. References "],
["conditional-structure.html", "Chapter 3 Conditional structure 3.1 Partial Dependence Plots 3.2 Individual Conditional Expectation Plot 3.3 Factor Merger 3.4 ALEPlot", " Chapter 3 Conditional structure 3.1 Partial Dependence Plots Partial Dependence Plots (see pdp package (Greenwell 2017)) for a black box \\(f(x; \\theta)\\) calculates the expected output given a selected variable. \\[ p_i(x_i) = E_{x_{-i}}[ f(x^i, x^{-i}; \\theta) ] \\] Of course this expectation cannot be calculated directly as we do not know fully the \\(f()\\) neither the distribution of \\(x_{-i}\\). This value is estimated by \\[ \\hat p_i(x_i) = \\frac{1}{n} \\sum_{j=1}^{n} f(x^i_j, x_j^{-i}, \\hat \\theta) \\] Let’s see an example for the model HR_rf_model. Below we are using pdp::partial function to calculate pdp curve for variable satisfaction_level. Then the curve is plotted with plotPartial (based on lattice) or autoplot (based on ggplot2). library(&quot;pdp&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) part &lt;- partial(HR_rf_model, &quot;satisfaction_level&quot;) plotPartial(part) library(&quot;ggplot2&quot;) autoplot(part) 3.2 Individual Conditional Expectation Plot Individual Conditional Expectations (ICE) may be considered as an extension of the PDP curves (see ICEbox package (Goldstein et al. 2015)). Instead of plotting expected value over all observations, for ICE we are plotting individual conditional model responses. Average of ICE curves results in PDP curve. An ICE curve for observation \\(k\\) over variable \\(i\\) may be defined as \\[ ice_k(x_i) = f(x^i, x_k^{-i}; \\theta) \\] ICE curves can be plotted with pdp package. Note that curves may be cantered in a given point, this helps in identification of possible interactions. library(&quot;pdp&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) library(&quot;ggplot2&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) part_rf_satisfaction &lt;- partial(HR_rf_model, &quot;satisfaction_level&quot;) part_rf_satisfaction &lt;- partial(HR_rf_model, pred.var = &quot;satisfaction_level&quot;, ice = TRUE) plotPartial(part_rf_satisfaction, rug = TRUE, train = HR_data, alpha = 0.2) autoplot(part_rf_satisfaction, center = TRUE, alpha = 0.2, rug = TRUE, train = HR_data) Or with the ICEbox package. library(&quot;ICEbox&quot;) part_rf_satisfaction = ice(object = HR_rf_model, X = HR_data, y = HR_data$satisfaction_level, predictor = &quot;satisfaction_level&quot;, frac_to_build = .1) ## ............................................................................................ plot(part_rf_satisfaction) As ICE curves are useful tool for identification of interactions, these individual curves may be clustered with the clusterICE function. clusterICE(part_rf_satisfaction, nClusters = 3, plot_legend = TRUE, center = TRUE) 3.3 Factor Merger factorMerger package (Sitko and Biecek 2017) 3.4 ALEPlot ALEPlot package (Apley 2017) References "],
["local-structure.html", "Chapter 4 Local structure 4.1 Local Interpretable (Model-agnostic) Visual Explanations 4.2 breakDown", " Chapter 4 Local structure Explainers presented in this chapter are designed to better understand the local structure of a black box in a single point. Example applications: explanations for predictions. Can be used to validate if a specific prediction is not accidental, is it based on variables important in the domain. examination of curvature around a specific point (single observation). Can be used to determine the strength of influence onto a final model. Is it an outlier? There are more interesting applications. Find out some of them in the Why Should I Trust You? article (Ribeiro, Singh, and Guestrin 2016). 4.1 Local Interpretable (Model-agnostic) Visual Explanations The live package (see (Staniak and Biecek 2017)) may be seen as an extension of the lime method (see (Ribeiro, Singh, and Guestrin 2016)). It is based on mlr general framework for training of machine learning models (see more (Bischl et al. 2016)). Let’s see an example. We will use the HR_rf_model trained with the randomForest package on Human Resources Analytics data. Around a selected point we will fit a linear model. library(&quot;live&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) similar &lt;- sample_locally(data = HR_data, explained_instance = HR_data[2,], explained_var = &quot;left&quot;, size = 2000) similar &lt;- add_predictions(HR_data, similar, HR_rf_model) trained &lt;- fit_explanation( live_object = similar, white_box = &quot;regr.lm&quot;, selection = FALSE) Fitted model may be plotted with waterfall plot … plot_explanation(trained, &quot;waterfallplot&quot;, explained_instance = HR_data[1,]) Figure 4.1: Waterfall plot created with live::plot_explanation … or forest plot … plot_explanation(trained, &quot;forestplot&quot;, explained_instance = HR_data[1,]) Figure 4.2: Fotest plot created with live::plot_explanation For more details consult the following vignette. 4.2 breakDown (P. Biecek 2017) References "],
["model-performance.html", "Chapter 5 Model performance", " Chapter 5 Model performance We have finished a nice book. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter ??. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 5.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 5.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 5.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 5.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2017) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "]
]
