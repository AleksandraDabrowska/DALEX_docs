[
["index.html", "DALEX: Descriptive mAchine Learning EXplanations Chapter 1 Introduction 1.1 Motivation 1.2 Notation 1.3 Use case - Human Resources Analytics 1.4 Use case - Wine quality 1.5 Trivia", " DALEX: Descriptive mAchine Learning EXplanations MI2DataLab 2018-02-13 Chapter 1 Introduction Machine Learning models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides. In many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. DALEX is a set of tools that help to understand how complex models are working. 1.1 Motivation Scheme (Štrumbelj and Kononenko 2011), (Tzeng and Ma 2005), (Puri et al. 2017), (Zeiler and Fergus 2014) 1.2 Notation This book describes explainers for different machine learning models. Some of these explainers are created by different research groups with different applications in mind. To keep the notation consistent: \\(x = (x_1, ..., x_p)\\) stands for a vector of \\(p\\) variables/predictors. \\(y\\) stands for a vector with target variable. In some applications it will be a continuous variable in others it will be a binary variable. \\(n\\) stands for number of observations. \\(f(x, \\theta)\\) stands for a model. We are considering models that are characterized by a set of parameters \\(\\theta\\). In some applications \\(\\theta\\) is a low level space of parameters - nice parametric models, in some applications \\(\\theta\\) may have a very complex structure. \\(\\lambda\\) stands for model meta-parameters which are not being directly optimized (like number of trees, max depth, some penalties etc.). \\(g(x)\\) stands for a function that pre-process variables. In some applications it may be a standardisation or other pre-processing. 1.3 Use case - Human Resources Analytics To ilustrate applications of DALEX to binary classification problems we will use a dataset from Kaggle competition Human Resources Analytics. This dataset is avaliable in the breakDown package (P. Biecek 2017). library(&quot;breakDown&quot;) head(HR_data) (#tab:hr_data)HR_data dataset from Kaggle competition Human Resources Analytics satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years sales salary 0.38 0.53 2 157 3 0 1 0 sales low 0.80 0.86 5 262 6 0 1 0 sales medium 0.11 0.88 7 272 4 0 1 0 sales medium 0.72 0.87 5 223 5 0 1 0 sales low 0.37 0.52 2 159 3 0 1 0 sales low 0.41 0.50 2 153 3 0 1 0 sales low 1.3.1 Logistic regression In the following chapters to present explainers for logistic regression models we will use HR_glm_model. HR_glm_model &lt;- glm(left~., data = HR_data, family = &quot;binomial&quot;) summary(HR_glm_model) ## ## Call: ## glm(formula = left ~ ., family = &quot;binomial&quot;, data = HR_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2248 -0.6645 -0.4026 -0.1177 3.0688 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.4762862 0.1938373 -7.616 2.61e-14 *** ## satisfaction_level -4.1356889 0.0980538 -42.178 &lt; 2e-16 *** ## last_evaluation 0.7309032 0.1491787 4.900 9.61e-07 *** ## number_project -0.3150787 0.0213248 -14.775 &lt; 2e-16 *** ## average_montly_hours 0.0044603 0.0005161 8.643 &lt; 2e-16 *** ## time_spend_company 0.2677537 0.0155736 17.193 &lt; 2e-16 *** ## Work_accident -1.5298283 0.0895473 -17.084 &lt; 2e-16 *** ## promotion_last_5years -1.4301364 0.2574958 -5.554 2.79e-08 *** ## saleshr 0.2323779 0.1313084 1.770 0.07678 . ## salesIT -0.1807179 0.1221276 -1.480 0.13894 ## salesmanagement -0.4484236 0.1598254 -2.806 0.00502 ** ## salesmarketing -0.0120882 0.1319304 -0.092 0.92700 ## salesproduct_mng -0.1532529 0.1301538 -1.177 0.23901 ## salesRandD -0.5823659 0.1448848 -4.020 5.83e-05 *** ## salessales -0.0387859 0.1024006 -0.379 0.70486 ## salessupport 0.0500251 0.1092834 0.458 0.64713 ## salestechnical 0.0701464 0.1065379 0.658 0.51027 ## salarylow 1.9440627 0.1286272 15.114 &lt; 2e-16 *** ## salarymedium 1.4132244 0.1293534 10.925 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16465 on 14998 degrees of freedom ## Residual deviance: 12850 on 14980 degrees of freedom ## AIC: 12888 ## ## Number of Fisher Scoring iterations: 5 Models used in this doccumentation are accessible via archivist package. To download the HR_glm_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/8fe19a108faf3ddfcabc3de3a0693234&quot;) 1.3.2 Random forest In the following chapters to present explainers for random forest models we will use HR_fr_model. library(&quot;randomForest&quot;) set.seed(1313) HR_data$left &lt;- factor(HR_data$left) HR_rf_model &lt;- randomForest(left~., data = HR_data, ntree = 100) HR_rf_model ## ## Call: ## randomForest(formula = left ~ ., data = HR_data, ntree = 100) ## Type of random forest: classification ## Number of trees: 100 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 0.75% ## Confusion matrix: ## 0 1 class.error ## 0 11407 21 0.001837592 ## 1 91 3480 0.025483058 Models used in this doccumentation are accessible via archivist package. To download the HR_rf_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/419d550a92fab6a5f28650130991e2cd&quot;) 1.4 Use case - Wine quality To ilustrate applications of DALEX to regression problems we will use a Wine quality dataset from Kaggle competition UC Irvine Machine Learning Repository. url &lt;- &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&#39; wine &lt;- read.table(url, header = TRUE, sep = &quot;;&quot;) head(wine) Table 1.1: Wine quality dataset from UC Irvine Machine Learning Repository fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality 7.0 0.27 0.36 20.7 0.045 45 170 1.0010 3.00 0.45 8.8 6 6.3 0.30 0.34 1.6 0.049 14 132 0.9940 3.30 0.49 9.5 6 8.1 0.28 0.40 6.9 0.050 30 97 0.9951 3.26 0.44 10.1 6 7.2 0.23 0.32 8.5 0.058 47 186 0.9956 3.19 0.40 9.9 6 7.2 0.23 0.32 8.5 0.058 47 186 0.9956 3.19 0.40 9.9 6 8.1 0.28 0.40 6.9 0.050 30 97 0.9951 3.26 0.44 10.1 6 1.4.1 Linear regression In the following chapters to present explainers for gaussian regression models we will use wine_lm_model. wine_lm_model &lt;- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, data = wine) Models used in this doccumentation are accessible via archivist package. To download the wine_lm_model model use the following instruction. archivist::aread(&quot;pbiecek/DALEX/arepo/b99a3d58016e2677221019652cff047f&quot;) 1.5 Trivia The Daleks are a fictional extraterrestrial race portrayed in the Doctor Who BBC series. Rather dim aliens, known to repeat the phrase Explain! very often. References "],
["global-structure.html", "Chapter 2 Global structure 2.1 Forest plots 2.2 Variable Importance Plot 2.3 ctree 2.4 RandomForestExplainer", " Chapter 2 Global structure Explainers presented in this chapter are designed to better understand the global structure of a black box. Which variables are the most important? How do they influence the final result of a model? 2.1 Forest plots Forest plots were initially used in the meta analysis to visualise effects in different studies. But now they are frequently used to present summary characteristics for models with linear structure like these created with lm or glm functions. There are various implementations of forest plots in R. Below we present examples for a glm model. library(&quot;breakDown&quot;) HR_glm_model &lt;- glm(left~., data = HR_data, family = &quot;binomial&quot;) #HR_glm_model &lt;- archivist::aread(&quot;pbiecek/DALEX/arepo/8fe19a108faf3ddfcabc3de3a0693234&quot;) In the package forestmodel (see (Kennedy 2017)) one can use forest_model() function to draw a forest plot. This package is based on the broom package (see (Robinson 2017)) and this is why it handles a large variety of different regression models. An example for glm. library(&quot;forestmodel&quot;) forest_model(HR_glm_model) Figure 2.1: Forest plot created with forestmodel package In the package sjPlot (see (Lüdecke 2017)) one can use sjp.*() to visualise structure of a * model or a wrapper plot_model(). Here is an example for glm model. library(&quot;sjPlot&quot;) plot_model(HR_glm_model, type = &quot;est&quot;, sort.est = TRUE) Figure 2.2: Forest plot created with sjPlot package Note! The forestmodel package handles factor variables in a better way while the plots from sjPlot are easier to read. 2.2 Variable Importance Plot 2.3 ctree 2.4 RandomForestExplainer References "],
["conditional-structure.html", "Chapter 3 Conditional structure 3.1 Partial Dependence Plot 3.2 Accumulated Local Effects Plot 3.3 Individual Conditional Expectation Plot 3.4 Mering Path Plot", " Chapter 3 Conditional structure Cheatsheet The dimension of input \\(x\\) for black box models is usually high (large \\(p\\)). But in many cases small number of variables play important role in the model OR there are some reasons to believe that variables work in an additive fashion/low-level interactions in the model. In such cases one may be interesting in verification how the conditional response for a selected interesting variable/variables looks like. Methods presented below help to understand the conditional structure of a model. 3.1 Partial Dependence Plot Partial Dependence Plots (see pdp package (Greenwell 2017)) for a black box \\(f(x; \\theta)\\) calculates the expected output given a selected variable. \\[ p_i(x_i) = E_{x_{-i}}[ f(x^i, x^{-i}; \\theta) ] \\] Of course this expectation cannot be calculated directly as we do not know fully the \\(f()\\) neither the distribution of \\(x_{-i}\\). This value is estimated by \\[ \\hat p_i(x_i) = \\frac{1}{n} \\sum_{j=1}^{n} f(x^i_j, x_j^{-i}, \\hat \\theta) \\] Let’s see an example for the model HR_rf_model. Below we are using DALEX::single_variable function that is calling pdp::partial function to calculate pdp curve for variable satisfaction_level. Then the curve is plotted with generic plot.single_variable_explainer() function. Marginal response plots are created in four steps. We need to fit model. For example a Random Forest model. library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) # a79f3c7ec27499fb91e46ee70d423ac8 # archivist::aread(&quot;pbiecek/DALEX/arepo/a79f3c7ec27&quot;) We need to create an explainer. It’s an interface that can be used to explain a black-box model. library(&quot;DALEX&quot;) explainer_rf &lt;- explain(HR_rf_model, data = HR_data) Now we can calculate the marginal response with the PDP method. expl_rf &lt;- single_variable(explainer_rf, variable = &quot;satisfaction_level&quot;, type = &quot;pdp&quot;) And we are ready to plot it. plot(expl_rf) # ad0f1699de646c78a46a3bf23aeea799 # archivist::aread(&quot;pbiecek/DALEX/arepo/ad0f1699&quot;) 3.1.1 Model Comparisons Marginal response plots are very useful in comparisons of different models. Let’s fit Generalized Linear Model, Random Forest Model and XGBoost Model to the same data. Then we can use PDP plots to compare these models. Random Forest Model was fitted in the previous chapter. Here we are training remaining models. HR_glm_model &lt;- glm(left~., data = breakDown::HR_data, family = &quot;binomial&quot;) library(&quot;xgboost&quot;) model_martix_train &lt;- model.matrix(left~.-1, breakDown::HR_data) data_train &lt;- xgb.DMatrix(model_martix_train, label = breakDown::HR_data$left) param &lt;- list(max_depth = 2, eta = 1, silent = 1, nthread = 2, objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) HR_xgb_model &lt;- xgb.train(param, data_train, nrounds = 50) Models are trained. Now we can create explainers and single variable explanations logit &lt;- function(x) exp(x)/(1+exp(x)) explainer_glm &lt;- explain(HR_glm_model, data = HR_data) expl_glm &lt;- single_variable(explainer_glm, &quot;satisfaction_level&quot;, &quot;pdp&quot;, trans=logit) explainer_xgb &lt;- explain(HR_xgb_model, data = model_martix_train) expl_xgb &lt;- single_variable(explainer_xgb, &quot;satisfaction_level&quot;, &quot;pdp&quot;, trans=logit) In order to compare these models it’s enough to plot all of them into a single chart. plot(expl_rf, expl_glm, expl_xgb) 3.2 Accumulated Local Effects Plot As it is presented in the chapter @(pdpchapter), the Partial Dependence Plot presents the expected model response with respect to marginal distribution of \\(x_{-i}\\). In some cases, e.g. when repressors are highly correlated, expectation over the marginal distribution may lead to biases/poorly extrapolated model responses. Especially in area far from the training set (see (Apley 2017) for more details). Accumulated local effects (ALE) plots (see ALEPlot package (Apley 2017)) solves this problem by using conditional distribution \\(x_{-i}|x_i = x_i^*\\). This leads to more stable and reliable estimates (at least when the predictors are highly correlated). Let see an example for ALE plots. We can used the model and explainer created in steps 1-2 in the PDP chapter above. Estimation of main effects for satisfaction_level is similar to the PDP curves. Here we are using DALEX::single_variable function that is calling ALEPlot::ALEPlot function to calculate ALE curve for variable satisfaction_level. exel_rf &lt;- single_variable(explainer_rf, variable = &quot;satisfaction_level&quot;, type = &quot;ale&quot;) plot(exel_rf) It may be useful to compare ALEPlots and PDP plots. Again, it’s simple with the generic DALEX function. plot(expl_rf, exel_rf) 3.3 Individual Conditional Expectation Plot Individual Conditional Expectations (ICE) may be considered as an extension of the PDP curves (see ICEbox package (Goldstein et al. 2015)). Instead of plotting expected value over all observations, for ICE we are plotting individual conditional model responses. Average of ICE curves results in PDP curve. An ICE curve for observation \\(k\\) over variable \\(i\\) may be defined as \\[ ice_k(x_i) = f(x^i, x_k^{-i}; \\theta) \\] ICE curves can be plotted with pdp package. Note that curves may be cantered in a given point, this helps in identification of possible interactions. library(&quot;pdp&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) library(&quot;ggplot2&quot;) HR_rf_model &lt;- randomForest(left~., data = breakDown::HR_data, ntree = 100) part_rf_satisfaction &lt;- partial(HR_rf_model, &quot;satisfaction_level&quot;) part_rf_satisfaction &lt;- partial(HR_rf_model, pred.var = &quot;satisfaction_level&quot;, ice = TRUE) plotPartial(part_rf_satisfaction, rug = TRUE, train = HR_data, alpha = 0.2) autoplot(part_rf_satisfaction, center = TRUE, alpha = 0.2, rug = TRUE, train = HR_data) Or with the ICEbox package. library(&quot;ICEbox&quot;) part_rf_satisfaction = ice(object = HR_rf_model, X = HR_data, y = HR_data$satisfaction_level, predictor = &quot;satisfaction_level&quot;, frac_to_build = .1) ## ............................................................................................ plot(part_rf_satisfaction) As ICE curves are useful tool for identification of interactions, these individual curves may be clustered with the clusterICE function. clusterICE(part_rf_satisfaction, nClusters = 3, plot_legend = TRUE, center = TRUE) 3.4 Mering Path Plot The package ICEbox is not working for factor variables while the pdp package returns plots that are hard to interpret. An interesting tool that helps to understand what is happening with factor variables is the factorMerger package (see (Sitko and Biecek 2017)). Here we have Merging Path Plot for a factor variable sales. library(&quot;factorMerger&quot;) path &lt;- mergeFactors(HR_data$left, HR_data$sales, method = &quot;fast-adaptive&quot;, family = &quot;binomial&quot;, abbreviate = FALSE) plot(path, panel = &quot;response&quot;) + theme_mi2() Note that you can use the factorMerger package to understand predictions calculated with a black-box model. The random forest model HR_rf_model returns continuous response. But the factorMerger works for such variables as well. In the top right panel one may see the distribution of predictions for the selected group. HR_data$left_predicted &lt;- predict(HR_rf_model) path &lt;- mergeFactors(HR_data$left_predicted, HR_data$sales, method = &quot;fast-adaptive&quot;, abbreviate = FALSE) plot(path, panel = &quot;response&quot;, responsePanel = &quot;boxplot&quot;, nodesSpacing = &quot;effects&quot;) + theme_mi2() References "],
["local-structure.html", "Chapter 4 Local structure 4.1 Local Interpretable (Model-agnostic) Visual Explanations 4.2 breakDown", " Chapter 4 Local structure Explainers presented in this chapter are designed to better understand the local structure of a black box in a single point. Example applications: explanations for predictions. Can be used to validate if a specific prediction is not accidental, is it based on variables important in the domain. examination of curvature around a specific point (single observation). Can be used to determine the strength of influence onto a final model. Is it an outlier? There are more interesting applications. Find out some of them in the Why Should I Trust You? article (Ribeiro, Singh, and Guestrin 2016). 4.0.1 Reason Most ML algorithms do not learn from mistakes. One calculates predictions and there is no room for improvement. But! The local predictions can change that! Understanding what causes wrong decisions may lead to model improvements. After all, if our prediction is wrong we shall update the model. 4.1 Local Interpretable (Model-agnostic) Visual Explanations The live package (see (Staniak and Biecek 2017)) may be seen as an extension of the lime method (see (Ribeiro, Singh, and Guestrin 2016)). It is based on mlr general framework for training of machine learning models (see more (Bischl et al. 2016)). Let’s see an example. We will use the HR_rf_model trained with the randomForest package on Human Resources Analytics data. Around a selected point we will fit a linear model. library(&quot;live&quot;) library(&quot;randomForest&quot;) library(&quot;breakDown&quot;) HR_data$left &lt;- as.numeric(as.character(HR_data$left)) HR_rf_model &lt;- randomForest(left~., data = HR_data, ntree=100) similar &lt;- sample_locally(data = HR_data, explained_instance = HR_data[1,], explained_var = &quot;left&quot;, size = 2000) similar &lt;- add_predictions(HR_data, similar, HR_rf_model) trained &lt;- fit_explanation( live_object = similar, white_box = &quot;regr.lm&quot;, selection = FALSE) Fitted model may be plotted with waterfall plot … plot_explanation(trained, &quot;waterfallplot&quot;, explained_instance = HR_data[1,]) … or forest plot … plot_explanation(trained, &quot;forestplot&quot;, explained_instance = HR_data[1,]) For more details consult the following vignette. 4.2 breakDown (P. Biecek 2017) References "],
["model-performance.html", "Chapter 5 Model performance 5.1 ROC 5.2 Lift curve", " Chapter 5 Model performance 5.1 ROC 5.2 Lift curve "],
["goodness-of-fit-model-diagnostic.html", "Chapter 6 Goodness of fit / Model diagnostic 6.1 halfnorm plot", " Chapter 6 Goodness of fit / Model diagnostic 6.1 halfnorm plot "],
["reproducibility.html", "Chapter 7 Reproducibility", " Chapter 7 Reproducibility Packages are changing quite fast, especially in actively developed areas. Below you will find list of packages that were installed on my computer when I was preparing this documentation. It is likely that some of described packages will change names of functions or arguments or structure of results. Use the version listed below to reproduce results form this book. Note, that results, models and plots created in are were recorded with the archivist package (P. Biecek and Kosinski 2017). Use archivist links to retrieve their binary copies directly to your R console. devtools::session_info() ## setting value ## version R version 3.4.3 (2017-11-30) ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz Asia/Singapore ## date 2018-02-13 ## ## package * version date source ## abind 1.4-5 2016-07-21 cran (@1.4-5) ## agricolae 1.2-8 2017-09-12 cran (@1.2-8) ## ALEPlot 1.0 2017-11-13 CRAN (R 3.4.2) ## AlgDesign 1.1-7.3 2014-10-15 CRAN (R 3.2.0) ## arm 1.9-3 2016-11-27 CRAN (R 3.4.0) ## assertthat 0.2.0 2017-04-11 CRAN (R 3.4.0) ## backports 1.1.1 2017-09-25 CRAN (R 3.4.2) ## base * 3.4.3 2017-12-07 local ## bayesplot 1.4.0 2017-09-12 CRAN (R 3.4.3) ## BBmisc 1.11 2017-03-10 cran (@1.11) ## bindr 0.1 2016-11-13 CRAN (R 3.4.0) ## bindrcpp * 0.2 2017-06-17 CRAN (R 3.4.0) ## blme 1.0-4 2015-06-14 CRAN (R 3.4.0) ## bookdown 0.5 2017-08-20 CRAN (R 3.4.1) ## boot 1.3-20 2017-08-06 CRAN (R 3.4.3) ## breakDown * 0.1.3 2018-01-27 local (pbiecek/breakDown@NA) ## broom 0.4.2 2017-02-13 CRAN (R 3.4.0) ## carData 3.0-0 2017-08-28 CRAN (R 3.4.1) ## checkmate 1.8.4 2017-09-25 cran (@1.8.4) ## cli 1.0.0 2017-11-05 cran (@1.0.0) ## cluster 2.0.6 2017-03-10 CRAN (R 3.4.3) ## coda 0.19-1 2016-12-08 cran (@0.19-1) ## codetools 0.2-15 2016-10-05 CRAN (R 3.4.3) ## coin 1.2-2 2017-11-28 cran (@1.2-2) ## colorspace 1.3-2 2016-12-14 CRAN (R 3.4.0) ## combinat 0.0-8 2012-10-29 CRAN (R 3.1.0) ## compiler 3.4.3 2017-12-07 local ## cowplot 0.9.1 2017-11-16 cran (@0.9.1) ## crayon 1.3.4 2017-09-16 CRAN (R 3.4.1) ## DALEX * 0.1 2018-02-12 local (@0.1) ## data.table 1.10.4-3 2017-10-27 CRAN (R 3.4.2) ## datasets * 3.4.3 2017-12-07 local ## deldir 0.1-14 2017-04-22 cran (@0.1-14) ## devtools 1.13.3 2017-08-02 CRAN (R 3.4.1) ## digest 0.6.13 2017-12-14 cran (@0.6.13) ## dplyr 0.7.4 2017-09-28 CRAN (R 3.4.2) ## DT 0.1 2015-06-09 CRAN (R 3.2.0) ## effects 4.0-0 2017-09-15 CRAN (R 3.4.1) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.4.1) ## expm 0.999-2 2017-03-29 cran (@0.999-2) ## factorMerger * 0.3.5 2018-01-11 local (geneticsMiNIng/FactorMerger@NA) ## forcats 0.2.0 2017-01-23 CRAN (R 3.4.0) ## foreign 0.8-69 2017-06-22 CRAN (R 3.4.3) ## forestmodel * 0.4.3 2017-04-16 CRAN (R 3.4.0) ## gdata 2.17.0 2015-07-04 CRAN (R 3.2.0) ## ggeffects 0.3.0 2017-11-27 CRAN (R 3.4.3) ## ggplot2 * 2.2.1 2016-12-30 CRAN (R 3.4.0) ## ggpubr 0.1.6 2017-11-14 cran (@0.1.6) ## glmmTMB 0.2.0 2017-12-11 CRAN (R 3.4.3) ## glue 1.1.1 2017-06-21 CRAN (R 3.4.1) ## gmodels 2.16.2 2015-07-22 CRAN (R 3.4.0) ## graphics * 3.4.3 2017-12-07 local ## grDevices * 3.4.3 2017-12-07 local ## grid 3.4.3 2017-12-07 local ## gridExtra 2.3 2017-09-09 CRAN (R 3.4.1) ## gtable 0.2.0 2016-02-26 CRAN (R 3.2.3) ## gtools 3.5.0 2015-05-29 CRAN (R 3.2.0) ## haven 1.1.0 2017-07-09 CRAN (R 3.4.1) ## highr 0.6 2016-05-09 CRAN (R 3.4.0) ## htmltools 0.3.6 2017-04-28 CRAN (R 3.4.0) ## htmlwidgets 0.9 2017-07-10 CRAN (R 3.4.1) ## httpuv 1.3.5 2017-07-04 CRAN (R 3.4.1) ## ICEbox * 1.1.2 2017-07-13 CRAN (R 3.4.1) ## klaR 0.6-12 2014-08-06 CRAN (R 3.2.0) ## knitr 1.18 2017-12-27 cran (@1.18) ## labeling 0.3 2014-08-23 CRAN (R 3.2.2) ## lattice 0.20-35 2017-03-25 CRAN (R 3.4.3) ## lazyeval 0.2.0 2016-06-12 CRAN (R 3.4.0) ## LearnBayes 2.15 2014-05-29 CRAN (R 3.2.0) ## live * 1.3.2 2018-01-25 local (MI2DataLab/live@f38ef48) ## lme4 1.1-15 2017-12-21 cran (@1.1-15) ## lmtest 0.9-34 2015-06-06 CRAN (R 3.2.0) ## lubridate 1.7.1 2017-11-03 cran (@1.7.1) ## magrittr 1.5 2014-11-22 CRAN (R 3.2.2) ## MASS 7.3-47 2017-02-26 CRAN (R 3.4.3) ## Matrix 1.2-12 2017-11-15 CRAN (R 3.4.2) ## memoise 1.1.0 2017-04-21 CRAN (R 3.4.0) ## merTools 0.3.0 2016-12-12 CRAN (R 3.4.0) ## methods * 3.4.3 2017-12-07 local ## mime 0.5 2016-07-07 CRAN (R 3.4.0) ## minqa 1.2.4 2014-10-09 CRAN (R 3.1.1) ## mlr * 2.11 2017-03-15 cran (@2.11) ## mnormt 1.5-5 2016-10-15 CRAN (R 3.4.0) ## modelr 0.1.1 2017-07-24 CRAN (R 3.4.1) ## modeltools 0.2-21 2013-09-02 CRAN (R 3.1.2) ## multcomp 1.4-8 2017-11-08 cran (@1.4-8) ## munsell 0.4.3 2016-02-13 CRAN (R 3.2.3) ## mvtnorm 1.0-6 2017-03-02 CRAN (R 3.4.0) ## nlme 3.1-131 2017-02-06 CRAN (R 3.4.3) ## nloptr 1.0.4 2014-08-04 CRAN (R 3.2.2) ## nnet 7.3-12 2016-02-02 CRAN (R 3.4.3) ## parallel 3.4.3 2017-12-07 local ## parallelMap 1.3 2015-06-10 cran (@1.3) ## ParamHelpers * 1.10 2017-01-05 cran (@1.10) ## pdp * 0.6.0 2017-07-20 CRAN (R 3.4.1) ## pillar 1.1.0 2018-01-14 cran (@1.1.0) ## pkgconfig 2.0.1 2017-03-21 CRAN (R 3.4.0) ## plyr 1.8.4 2016-06-08 CRAN (R 3.4.0) ## prediction 0.2.0 2017-04-19 CRAN (R 3.4.0) ## proxy 0.4-20 2017-12-12 cran (@0.4-20) ## psych 1.7.3.21 2017-03-22 CRAN (R 3.4.0) ## purrr 0.2.4 2017-10-18 cran (@0.2.4) ## pwr 1.2-1 2017-03-25 CRAN (R 3.4.0) ## R6 2.2.2 2017-06-17 CRAN (R 3.4.0) ## randomForest * 4.6-12 2015-10-07 CRAN (R 3.2.0) ## ranger 0.9.0 2018-01-09 cran (@0.9.0) ## RColorBrewer 1.1-2 2014-12-07 CRAN (R 3.2.2) ## Rcpp 0.12.15 2018-01-20 cran (@0.12.15) ## reshape2 1.4.3 2017-12-11 cran (@1.4.3) ## rlang 0.1.6 2017-12-21 CRAN (R 3.4.3) ## rmarkdown 1.8 2017-11-17 cran (@1.8) ## rprojroot 1.2 2017-01-16 CRAN (R 3.4.0) ## rstudioapi 0.7 2017-09-07 CRAN (R 3.4.1) ## sandwich 2.4-0 2017-07-26 cran (@2.4-0) ## scales 0.5.0 2017-08-24 CRAN (R 3.4.1) ## sfsmisc * 1.1-1 2017-06-08 CRAN (R 3.4.0) ## shiny 1.0.5 2017-08-23 CRAN (R 3.4.1) ## sjlabelled 1.0.5 2017-11-09 CRAN (R 3.4.2) ## sjmisc 2.6.3 2017-11-28 CRAN (R 3.4.3) ## sjPlot * 2.4.0 2017-10-19 CRAN (R 3.4.2) ## sjstats 0.13.0 2017-11-23 CRAN (R 3.4.3) ## snakecase 0.5.1 2017-09-20 CRAN (R 3.4.2) ## sp 1.2-5 2017-06-29 cran (@1.2-5) ## spData 0.2.6.4 2017-11-12 cran (@0.2.6.4) ## spdep 0.7-4 2017-11-22 cran (@0.7-4) ## splines 3.4.3 2017-12-07 local ## stats * 3.4.3 2017-12-07 local ## stats4 3.4.3 2017-12-07 local ## stringdist 0.9.4.1 2016-01-02 CRAN (R 3.2.3) ## stringi 1.1.6 2017-11-17 cran (@1.1.6) ## stringr 1.2.0 2017-02-18 CRAN (R 3.4.0) ## survey 3.30-3 2014-08-15 CRAN (R 3.1.2) ## survival 2.41-3 2017-04-04 CRAN (R 3.4.3) ## TH.data 1.0-8 2017-01-23 cran (@1.0-8) ## tibble 1.4.2 2018-01-22 cran (@1.4.2) ## tidyr 0.7.2 2017-10-16 cran (@0.7.2) ## tidyselect 0.2.2 2017-10-10 cran (@0.2.2) ## TMB 1.7.12 2017-12-11 CRAN (R 3.4.3) ## tools 3.4.3 2017-12-07 local ## utils * 3.4.3 2017-12-07 local ## withr 2.1.0 2017-11-01 cran (@2.1.0) ## xgboost * 0.6-4 2017-01-05 cran (@0.6-4) ## xtable 1.8-2 2016-02-05 CRAN (R 3.2.3) ## yaImpute 1.0-29 2017-12-10 CRAN (R 3.4.3) ## yaml 2.1.16 2017-12-12 cran (@2.1.16) ## zoo 1.8-0 2017-04-12 CRAN (R 3.4.0) References "],
["hands-on-tutorial.html", "Chapter 8 Hands-on tutorial 8.1 Title 8.2 Outline 8.3 Keywords 8.4 Equipment 8.5 Target audience 8.6 Motivation", " Chapter 8 Hands-on tutorial 8.1 Title DALEX: Descriptive mAchine Learning EXplanations Tools for exploration, validation and explanation of complex machine learning models 8.2 Outline Complex machine learning models are frequently used in predictive modelling. There are a lot of examples for random forest like or boosting like models in medicine, finance, agriculture etc. In this workshop we will show why and how one would analyse the structure of the black-box model. This will be a hands-on workshop with four parts. In each part there will be a short lecture (around 20-25 minutes) and then time for practice and discussion (around 20-25 min). Introduction Here we will show what problems may arise from blind application of black-box models. Also we will show situations in which the understanding of a model structure leads to model improvements, model stability and larger trust in the model. During the hands-on part we will fit few complex models (like xgboost, randomForest) with the mlr package and discuss basic diagnostic tools for these models. Conditional Explainers In this part we will introduce techniques for understanding of marginal/conditional response of a model given a one- two- variables. We will cover PDP (Partial Dependence Plots) and ICE (Individual Conditional Expectations) packages for continuous variables and MPP (Merging Path Plot from factorMerger package) for categorical variables. Local Explainers In this part we will introduce techniques that explain key factors that drive single model predictions. This covers Break Down plots for linear models (lm / glm) and tree-based models (randomForestExplainer, xgboostExplainer) along with model agnostic approaches implemented in the live package (an extension of the LIME method). Global Explainers In this part we will introduce tools for global analysis of the black-box model, like variable importance plots, interaction importance plots and tools for model diagnostic. Literature Staniak, Mateusz, and Przemysław Biecek. 2017. Live: Local Interpretable (Model-Agnostic) Visual Explanations. Sitko, Agnieszka, and Przemyslaw Biecek. 2017. FactorMerger: Hierarchical Algorithm for Post-Hoc Testing. https://github.com/MI2DataLab/factorMerger. Greenwell, Brandon M. 2017. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36. https://journal.r-project.org/archive/2017/RJ-2017-016/index.html. Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” Journal of Computational and Graphical Statistics 24 (1): 44–65. doi:10.1080/10618600.2014.907095. Apley, Dan. 2017. ALEPlot: Accumulated Local Effects (Ale) Plots and Partial Dependence (Pd) Plots. https://CRAN.R-project.org/package=ALEPlot. Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier.” In, 1135–44. ACM Press. doi:10.1145/2939672.2939778. Biecek, Przemyslaw. 2017. BreakDown: BreakDown Plots. https://CRAN.R-project.org/package=breakDown. 8.3 Keywords machine learning, black-box model, model explainers, model auditing, model visualization 8.4 Equipment Laptop with preinstalled R and selected packages. All necessary models and datasets will be provided in advance. 8.5 Target audience Applied data scientists, analysts interested in machine learning models. 8.6 Motivation There are at least three good reasons why this tutoial will be interesting for conference participants. The topic is super interesting. Even short blog posts on R Bloggers (like http://smarterpoland.pl/index.php/2017/12/explain-explain-explain/) got lots of reactions. There is a large community around model explainer sin python (the ELI5 package) and there is growing community for R. I had a talk about explainers during the last UseR conference 2017 (https://user2017.sched.com/event-goers/10415a8ff5675f10deb3fd27b43db5a7) more than 500 users sign for it. We have developed consistent toolbox of R packages - called DALEX - that supports model explanations. Participants of the workshop will learn easy-to-use tools that can be applied in their every-day data analyses. Methods that we will describe work for most of popular ML models (random forest, glm, boosting). Anything related to Machine Learning brings a lot of participants. I’ve attended ML-related workshops during UseR 2015, UseR 2016 and UseR 2017. Demand for such tutorials was always the highest. This is why we expect a high interest in this tutorial. "]
]
