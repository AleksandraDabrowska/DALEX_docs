<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="DALEX: Descriptive mAchine Learning EXplanations" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Do not trust a black-box model. Unless it explains itself." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Przemysław Biecek" />

<meta name="date" content="2018-04-02" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Do not trust a black-box model. Unless it explains itself.">

<title>DALEX: Descriptive mAchine Learning EXplanations</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li class="has-sub"><a href="1-1-motivation.html#motivation"><span class="toc-section-number">1.1</span> Motivation</a><ul>
<li><a href="1-1-motivation.html#why-dalex"><span class="toc-section-number">1.1.1</span> Why DALEX?</a></li>
<li><a href="1-1-motivation.html#to-validate"><span class="toc-section-number">1.1.2</span> To validate</a></li>
<li><a href="1-1-motivation.html#to-understand"><span class="toc-section-number">1.1.3</span> To understand</a></li>
<li><a href="1-1-motivation.html#to-improve"><span class="toc-section-number">1.1.4</span> To improve</a></li>
</ul></li>
<li><a href="1-2-trivia.html#trivia"><span class="toc-section-number">1.2</span> Trivia</a></li>
</ul></li>
<li class="has-sub"><a href="2-architecture.html#architecture"><span class="toc-section-number">2</span> Architecture of DALEX</a><ul>
<li><a href="2-1-explainFunction.html#explainFunction"><span class="toc-section-number">2.1</span> The <code>explain()</code> function</a></li>
<li class="has-sub"><a href="2-2-use-case-regression-apartment-prices-in-warsaw.html#use-case-regression.-apartment-prices-in-warsaw"><span class="toc-section-number">2.2</span> Use case: Regression. Apartment prices in Warsaw</a><ul>
<li><a href="2-2-use-case-regression-apartment-prices-in-warsaw.html#linear-regression"><span class="toc-section-number">2.2.1</span> Linear regression</a></li>
<li><a href="2-2-use-case-regression-apartment-prices-in-warsaw.html#random-forest"><span class="toc-section-number">2.2.2</span> Random forest</a></li>
</ul></li>
<li><a href="2-3-use-case-classification-tbd.html#use-case-classification.-tbd"><span class="toc-section-number">2.3</span> Use case: Classification. TBD</a></li>
</ul></li>
<li class="has-sub"><a href="3-modelUnderstanding.html#modelUnderstanding"><span class="toc-section-number">3</span> Model understanding</a><ul>
<li><a href="3-1-modelPerformance.html#modelPerformance"><span class="toc-section-number">3.1</span> Model performance</a></li>
<li class="has-sub"><a href="3-2-featureImportance.html#featureImportance"><span class="toc-section-number">3.2</span> Feature importance</a><ul>
<li><a href="3-2-featureImportance.html#drop-out-plots"><span class="toc-section-number">3.2.1</span> Drop-out plots</a></li>
<li><a href="3-2-featureImportance.html#forest-plots"><span class="toc-section-number">3.2.2</span> Forest plots</a></li>
</ul></li>
<li class="has-sub"><a href="3-3-variableResponse.html#variableResponse"><span class="toc-section-number">3.3</span> Variable response</a><ul>
<li><a href="3-3-variableResponse.html#pdpchapter"><span class="toc-section-number">3.3.1</span> Partial Dependence Plot</a></li>
<li><a href="3-3-variableResponse.html#model-comparisons"><span class="toc-section-number">3.3.2</span> Model Comparisons</a></li>
<li><a href="3-3-variableResponse.html#accumulated-local-effects-plot"><span class="toc-section-number">3.3.3</span> Accumulated Local Effects Plot</a></li>
<li><a href="3-3-variableResponse.html#individual-conditional-expectation-plot"><span class="toc-section-number">3.3.4</span> Individual Conditional Expectation Plot</a></li>
<li><a href="3-3-variableResponse.html#mering-path-plot"><span class="toc-section-number">3.3.5</span> Mering Path Plot</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="4-predictionUnderstanding.html#predictionUnderstanding"><span class="toc-section-number">4</span> Prediction understanding</a><ul>
<li><a href="4-1-outlier-detection.html#outlier-detection"><span class="toc-section-number">4.1</span> Outlier detection</a></li>
<li class="has-sub"><a href="4-2-predictionBreakdown.html#predictionBreakdown"><span class="toc-section-number">4.2</span> Prediction breakdown</a><ul>
<li><a href="4-2-predictionBreakdown.html#basics"><span class="toc-section-number">4.2.1</span> Basics</a></li>
</ul></li>
<li><a href="4-3-local-interpretable-model-agnostic-visual-explanations.html#local-interpretable-model-agnostic-visual-explanations"><span class="toc-section-number">4.3</span> Local Interpretable (Model-agnostic) Visual Explanations</a></li>
<li class="has-sub"><a href="4-4-breakdown.html#breakdown"><span class="toc-section-number">4.4</span> breakDown</a><ul>
<li><a href="4-4-breakdown.html#model-comparisons-1"><span class="toc-section-number">4.4.1</span> Model Comparisons</a></li>
</ul></li>
</ul></li>
<li><a href="5-reproducibility.html#reproducibility"><span class="toc-section-number">5</span> Reproducibility</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="motivation" class="section level2">
<h2><span class="header-section-number">1.1</span> Motivation</h2>
<p><em>Machine Learning</em> is a vague name, there is some <em>learning</em> and some <em>machines</em>, but what the heck is going on?</p>
<p>Actually I like the vagueness, because the interpretation of the name evolves over time as the discipline does.</p>
<ul>
<li>Few years ago I would extend this name as <em>machines are learning from human</em>. In the supervised learning problems, human creates a labeled dataset and machines are tuned to predict labels from data.</li>
<li>Recently we have more and more examples of <em>machines that are learning from other machines</em>. Self playing neural net like AlphaGo Zero <span class="citation">(Silver et al. <label for="tufte-mn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle">2017<span class="marginnote">Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, et al. 2017. “Mastering the Game of Go Without Human Knowledge.” <em>Nature</em> 550 (7676):354–59. <a href="https://doi.org/10.1038/nature24270" class="uri">https://doi.org/10.1038/nature24270</a>.</span>)</span> learns from themself in a blazing speed. Still humans are involved in designing of the learning environment but the labelling turns out to be very expensive or not feasible and we are looking for other ways to learn from partial labels, fuzzy labels, or no labels at all.</li>
<li>I could imagine that in close future <em>humans will learn from machines</em>. Well trained black-boxes may tech us how to be a better in playing go, how to be better in reading PET images, or how to be better in writing of books.</li>
</ul>
<p>To make this future possible we need tools that extract useful information from black-box models. And as the human supervision over the learning is smaller over time, the black-box understanding is more important.</p>
<p>DALEX is <em>the tool</em> for this.</p>
<div id="why-dalex" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Why DALEX?</h3>
<p>In recent years there is a lot of works focused in knowledge extraction from complex machine learning models, especially trained with the deep learning techniques. See for example <span class="citation">(Štrumbelj and Kononenko <label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">2011<span class="marginnote">Štrumbelj, Erik, and Igor Kononenko. 2011. “A General Method for Visualizing and Explaining Black-Box Regression Models.” In <em>Proceedings of the 10th International Conference on Adaptive and Natural Computing Algorithms - Volume Part Ii</em>, 21–30. ICANNGA’11. Berlin, Heidelberg: Springer-Verlag. <a href="http://dl.acm.org/citation.cfm?id=1997005.1997009" class="uri">http://dl.acm.org/citation.cfm?id=1997005.1997009</a>.</span>)</span>, <span class="citation">(Tzeng and Ma <label for="tufte-mn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle">2005<span class="marginnote">Tzeng, F. Y., and K. L. Ma. 2005. “Opening the Black Box - Data Driven Visualization of Neural Networks.” In <em>VIS 05. IEEE Visualization, 2005.</em>, 383–90. <a href="https://doi.org/10.1109/VISUAL.2005.1532820" class="uri">https://doi.org/10.1109/VISUAL.2005.1532820</a>.</span>)</span>, <span class="citation">(Puri et al. <label for="tufte-mn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle">2017<span class="marginnote">Puri, N., P. Gupta, P. Agarwal, S. Verma, and B. Krishnamurthy. 2017. “MAGIX: Model Agnostic Globally Interpretable Explanations.” <em>ArXiv E-Prints</em>, June.</span>)</span>, <span class="citation">(Zeiler and Fergus <label for="tufte-mn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">2014<span class="marginnote">Zeiler, Matthew D., and Rob Fergus. 2014. “Visualizing and Understanding Convolutional Networks.” In <em>Computer Vision – Eccv 2014</em>, edited by David Fleet, Tomas Pajdla, Bernt Schiele, and TinneEditors Tuytelaars, 818–33. Springer International Publishing.</span>)</span>.</p>
<p>There are also some R packages that may be used for knowledge extraction from machine learning models. See for example <code>pdp</code> <span class="citation">(Greenwell <label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">2017<span class="marginnote">Greenwell, Brandon M. 2017. “Pdp: An R Package for Constructing Partial Dependence Plots.” <em>The R Journal</em> 9 (1):421–36. <a href="https://journal.r-project.org/archive/2017/RJ-2017-016/index.html" class="uri">https://journal.r-project.org/archive/2017/RJ-2017-016/index.html</a>.</span>)</span>, <code>ALEPlot</code> <span class="citation">(Apley <label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">2017<span class="marginnote">Apley, Dan. 2017. <em>ALEPlot: Accumulated Local Effects (Ale) Plots and Partial Dependence (Pd) Plots</em>. <a href="https://CRAN.R-project.org/package=ALEPlot" class="uri">https://CRAN.R-project.org/package=ALEPlot</a>.</span>)</span>, <code>randomForestExplainer</code> <span class="citation">(Paluszynska and Biecek <label for="tufte-mn-8" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">2017<span class="marginnote">Paluszynska, Aleksandra, and Przemyslaw Biecek. 2017. <em>RandomForestExplainer: A Set of Tools to Understand What Is Happening Inside a Random Forest</em>. <a href="https://github.com/MI2DataLab/randomForestExplainer" class="uri">https://github.com/MI2DataLab/randomForestExplainer</a>.</span>)</span>, <code>xgboostExplainer</code> <span class="citation">(Foster <label for="tufte-mn-9" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">2017<span class="marginnote">Foster, David. 2017. <em>XgboostExplainer: An R Package That Makes Xgboost Models Fully Interpretable</em>. <a href="https://github.com/AppliedDataSciencePartners/xgboostExplainer/" class="uri">https://github.com/AppliedDataSciencePartners/xgboostExplainer/</a>.</span>)</span>, <code>live</code> <span class="citation">(Staniak and Biecek <label for="tufte-mn-10" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle">2017<span class="marginnote">Staniak, Mateusz, and Przemyslaw Biecek. 2017. <em>Live: Local Interpretable (Model-Agnostic) Visual Explanations</em>. <a href="https://github.com/MI2DataLab/live" class="uri">https://github.com/MI2DataLab/live</a>.</span>)</span> and others.</p>
<p>So why do we need yet another R package for this? There are some unique features of the DALEX package.</p>
<ul>
<li>Scope. DALEX is a wrapper over large number of very good tools / model explainers. It offers a wide range of state of the art techniques for model exploration. Some of these techniques are more useful for understanding of model predictions, some are more useful in understanding of structure.</li>
<li>Consistency. DALEX offers a consistent grammar across various techniques for model explanation. It’s a wrapper that smooths differences across different dependent packages.</li>
<li>Model agnostic. DALEX explainers are model agnostic, we can use them for linear models, tree ensembles of other structures. So we are not limited to any particular family of black-box models.</li>
<li>Model comparisons. One can learn a lot from single black-box model, but actually we can learn much more by contrasting models with different structures, like linear models vs. ensembles of trees. All explainers in DALEX by default support model comparisons on various levels.</li>
<li>Visual consistency. Each DALEX explainer can be plotted with the generic <code>plot()</code> function. These visual explanations are based on <code>ggplot2</code> <span class="citation">(Wickham <label for="tufte-mn-11" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">2009<span class="marginnote">Wickham, Hadley. 2009. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="http://ggplot2.org" class="uri">http://ggplot2.org</a>.</span>)</span> package, which results in elegant, customizable, consistent graphs.</li>
</ul>
<p>Chapter <a href="2-architecture.html#architecture">2</a> presents the overall architecture of the DALEX package. Chapter <a href="3-modelUnderstanding.html#modelUnderstanding">3</a> presents explainers that explore global model performance, variable importance of feature effects. Chapter <a href="4-predictionUnderstanding.html#predictionUnderstanding"><strong>??</strong></a> presents explainers that explore feature attribution for single predictions of validation of the reliability of a model prediction.</p>
<p>In this document we will focus on three primary use-cases for DALEX explainers.</p>
</div>
<div id="to-validate" class="section level3">
<h3><span class="header-section-number">1.1.2</span> To validate</h3>
<p>Explainers presented in the section <a href="3-1-modelPerformance.html#modelPerformance">3.1</a> helps to understand model performance and compare different models on the same scale. The model comparison is richer that a one based on a single number and helps to understand model performance along the full range of model predictions.</p>
<p>Explainers presented in the section <a href="#outlierDetection"><strong>??</strong></a> helps to identify outliers or observations with particularly unusual value.</p>
<p>Explainers presented in the section <a href="4-2-predictionBreakdown.html#predictionBreakdown">4.2</a> helps to understand which features influence heavily model predictions.</p>
</div>
<div id="to-understand" class="section level3">
<h3><span class="header-section-number">1.1.3</span> To understand</h3>
<p>Explainers presented in the section <a href="3-2-featureImportance.html#featureImportance">3.2</a> helps to understand which variables are the most important in the model. Explainers presented in the section <a href="4-2-predictionBreakdown.html#predictionBreakdown">4.2</a> helps to understand which features influence single model predictions. They are useful to understand the key ingredients of the model.</p>
<p>Explainers presented in the section <a href="3-3-variableResponse.html#variableResponse">3.3</a> helps to understand how features affect model prediction.</p>
</div>
<div id="to-improve" class="section level3">
<h3><span class="header-section-number">1.1.4</span> To improve</h3>
<p>Explainers presented in the section <a href="3-3-variableResponse.html#variableResponse">3.3</a> helps to perform feature engineering based on feature marginal responses.</p>
<p>Explainers presented in the section <a href="4-2-predictionBreakdown.html#predictionBreakdown">4.2</a> helps to understand which variables affect incorrect model decisions. This is useful to identify and correct biases in the training data.</p>
</div>
</div>
<p style="text-align: center;">
<a href="index.html"><button class="btn btn-default">Previous</button></a>
<a href="https://github.com/pbiecek/DALEX/edit/master/index.Rmd"><button class="btn btn-default">Edit</button></a>
<a href="1-2-trivia.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
